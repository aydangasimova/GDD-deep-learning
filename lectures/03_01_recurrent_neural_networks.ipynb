{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "![footer_logo](../images/logo.png)\n",
    "\n",
    "## Goal\n",
    "\n",
    "We will discuss Recurrent neural networks and how they enable us to perform sequential tasks.\n",
    "\n",
    "## Program\n",
    "\n",
    "- [Sequences and neural networks]()\n",
    "- [Recurrent neural networks (RNNs)]()\n",
    "- [Applications]()\n",
    "- [Types of RNNs]()\n",
    "- [Training an RNN (BPTT)]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequential data \n",
    "\n",
    "### The order of a sequence holds information\n",
    "\n",
    "> proudly part of Xebia Group\n",
    "\n",
    "vs\n",
    "\n",
    "> Group part of Xebia proudly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Feed-forward üíî sequences\n",
    "\n",
    "We need a different kind of unit!\n",
    "\n",
    "![center quarter](../images/rnn/feedforward-sequence.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent ‚ù§Ô∏è sequences\n",
    "\n",
    "Internal loop feeds back the previous state \n",
    "\n",
    "<center><img src=\"../images/rnn/rnn-architecture.png\" width=\"800\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Formal comparison\n",
    "\n",
    "**Feed forward**\n",
    "\n",
    "<img src=\"../images/rnn/formula-feedforward.png\" style=\"height: 50px; display: block; margin-left: auto !important; margin-right: auto !important;\" align=\"center\"/>\n",
    "\n",
    "**Recurrent**\n",
    "\n",
    "<img src=\"../images/rnn/formula-recurrent.png\" style=\"height: 56px; display: block; margin-left: auto !important; margin-right: auto !important;\" align=\"center\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent neural network (RNN)\n",
    "\n",
    "- contains at least one feed-back connection\n",
    "- enables the neural network to do temporal processing and learn sequences\n",
    "\n",
    "![center quarter](../images/rnn/rnn_loop.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "<img src=\"../images/rnn/rnn_unit.png\" align='right' width='300'>\n",
    "\n",
    "- Proposed in the 80s for modeling time series\n",
    "\n",
    "- An RNN does not start its \"thinking\" from scratch\n",
    "\n",
    "- Networks can persist information\n",
    "\n",
    "- Take earlier inputs into account when making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "Network architectures:\n",
    "\n",
    "- Simple RNNs\n",
    "- Long short-term memory\n",
    "- Gated recurrent units\n",
    "\n",
    "Use cases:\n",
    "\n",
    "- Forecasting\n",
    "- Classification\n",
    "- Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications: Language translation\n",
    "\n",
    "![three_quarters center](../images/rnn/google-translate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications: Image captioning\n",
    "\n",
    "![half center](../images/rnn/image-captioning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications: Speech recognition\n",
    "\n",
    "![half center](../images/rnn/speech_recognition.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications: Session-based recommendations\n",
    "\n",
    "![center](../images/rnn/gru4rec.png)\n",
    "\n",
    "<sup>Source: [Hidasi et. al, 2015 \"\n",
    "Session-based Recommendations with Recurrent Neural Networks\"](https://arxiv.org/abs/1511.06939)<sup/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications: Sentiment analysis\n",
    "\n",
    "\n",
    "![center three_quarters](../images/rnn/sentiment-neuron.gif)\n",
    "\n",
    "<sup>Source: [Unsupervised Sentiment Neuron](https://blog.openai.com/unsupervised-sentiment-neuron/)<sup/>\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications: Entity recognition\n",
    "\n",
    "\n",
    "![center three_quarters](../images/rnn/entity_recognition.gif)\n",
    "\n",
    "<sup>Source: [DL experiment for a GDD client by Marcel Raas](https://godatadriven.com/players/marcel-raas)<sup/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling sequences\n",
    "\n",
    "- Feedforward: activation determined by the input\n",
    "- RNN: architecture contains loops\n",
    "- RNN: activation might be determined also by its own activation at an earlier time. \n",
    "\n",
    "> A RNN can be thought of as multiple copies of the same network linked through time\n",
    "\n",
    "![center three_quarters](../images/rnn/RNN_unrolled.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Types of RNNs\n",
    "\n",
    "- A RNN provides a natural and flexible architecture for modeling all kinds of sequence data\n",
    "\n",
    "![center half](../images/rnn/rnn_sequence.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One to many example task?\n",
    "\n",
    "![center](../images/rnn/rnn_one_to_many.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Image captioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Many to one example task?\n",
    "\n",
    "![center](../images/rnn/rnn_many_to_one.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Many to many example task?\n",
    "\n",
    "![center](../images/rnn/rnn_many_to_many_a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Many to many example task?\n",
    "\n",
    "![center](../images/rnn/rnn_many_to_many_b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Video frames classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training an RNN: Backpropagation through time ([BPTT](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf))\n",
    "\n",
    "Below is a simple RNN (many-to-one)\n",
    "\n",
    "![third center](../images/rnn/simple_recurrent.png)\n",
    "\n",
    "$$\\begin{align}\n",
    "a_t &= \\varphi(W_h\\cdot x_t + W_r\\cdot a_{t-1})\\\\\n",
    "y_t &= W_o\\cdot a_t\n",
    "\\end{align}$$\n",
    "<!-- \n",
    "- One-step ahead forecasting: $x_t\\rightarrow y_{t-1}$ -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training an RNN: Backpropagation through time ([BPTT](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf))\n",
    "\n",
    "To train an RNN we backpropogate the error through time.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\Sigma_t\\frac{\\partial L_t}{\\partial w}$$\n",
    "\n",
    "Backpropagation of the $\\delta$ error vectors through the network.\n",
    "\n",
    "![center third](../images/rnn/bptt_recurrent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parameter sharing\n",
    "\n",
    "Note that we do not have serperate weights at each time step\n",
    "\n",
    "![center third](../images/rnn/bptt_recurrent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Parameter sharing\n",
    "\n",
    "- It would require a lot of resources, if there parameters are not shared. \n",
    "- We can generalize to sequences of different lengths.\n",
    "- Reflects the fact that we are performing the same task at each step, as a result, we don't have to relearn the rules at each point in the sequnce\n",
    "- Oftentimes, components of a sequences operate the same across the sequence. For instance, in NLP:\n",
    "\n",
    "                                                     \"On Monday it was snowing\"\n",
    "\n",
    "                                                     \"It was snowing on Monday\"\n",
    "    *i.e. we observe order, not position.* \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we have covered,\n",
    "\n",
    "- The issues with sequences and standard neural networks.\n",
    "- Neural networks with a recurrent connection (RNNs) and their applications.\n",
    "- Training an RNN with BPTT and the importance of paramter sharing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN Exercise\n",
    "[Exercise: RNN forecast airline passengers](../exercises/03_01_rnn_forecast_airline_passenger.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
