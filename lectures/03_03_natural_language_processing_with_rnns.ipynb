{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "WORDS = [\"Jack\", \"likes\", \"Jill\", \"cat\", \"dog\"]\n",
    "DOCUMENTS = ['\"Jack likes Jill\"', '\"Jill likes Jack\"', '\"dog\"']\n",
    "EMBEDDINGS = [\"embedding_0\", \"embedding_1\"]\n",
    "def one_hot_encoding():\n",
    "    has = [\"has_\" + w for w in WORDS]\n",
    "    return pd.DataFrame(np.eye(len(WORDS), dtype=int), WORDS, columns=has)\n",
    "def one_hot_document():\n",
    "    has = [\"has_\" + w for w in WORDS]\n",
    "    return pd.DataFrame(\n",
    "        [[1, 1, 1, 0, 0], [1, 1, 1, 0, 0], [0, 0, 0, 0, 1]],\n",
    "        index=DOCUMENTS,\n",
    "        columns=has,\n",
    "    )\n",
    "def embedding_encoding():\n",
    "    return pd.DataFrame(\n",
    "        [[0, 0.4, 0.1, 0.7, 0.8], [0.1, 0.4, 0.1, 0.6, 0.7]],\n",
    "        index=EMBEDDINGS,\n",
    "        columns=WORDS,\n",
    "    ).T\n",
    "def embedding_document():\n",
    "    return pd.DataFrame(\n",
    "        [[0.5, 0.6], [0.5, 0.6], [0.8, 0.7]], index=DOCUMENTS, columns=EMBEDDINGS\n",
    "    )\n",
    "def embedding_plot(df):\n",
    "    return sns.scatterplot(data=df, x='embedding_0', y='embedding_1', hue=WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural language processing for RNNs\n",
    "\n",
    "![footer_logo](../images/logo.png)\n",
    "\n",
    "## Goal\n",
    "\n",
    "In this notebook we will discuss Natural Language Processing in the context of deep learning. We will discuss how to encode words in to feature vectors and then process them sequentially using RNNs/LSTMs.\n",
    "\n",
    "## Program\n",
    "\n",
    "- [Encoding words]()\n",
    "    - [One-hot enconding]()\n",
    "    - [Embeddings]()\n",
    "- [Word2vec]()\n",
    "- [fastText]()\n",
    "- [RNNs/LSTMs for NLP]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One-hot encoding\n",
    "\n",
    "- Traditional style of encoding\n",
    "- Represents words as a big vectors with zeros and ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Example vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One-hot encoding\n",
    "\n",
    "- Traditional style of encoding\n",
    "- Represents words as a big vectors with zeros and ones.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Example documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Disadvantages\n",
    "\n",
    "- Big and sparse vector space.\n",
    "- Too many features to learn from (i.e. could end up with more features than samples).\n",
    "- Doesn't capture relations between words (e.g. cat is more similar to dog than it is to Jack).\n",
    "- Treating words as atomic units throws away a lot of information (e.g. \"Jack likes Jill\" $\\neq$ \"Jill likes Jack\").\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embeddings are a better representation\n",
    "\n",
    "Smaller and dense vector space\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Example vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_encoding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Embeddings should capture the relationship between words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_plot(embedding_encoding());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "How can we learn these embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word2Vec \n",
    "\n",
    "With (Skip-gram) Word2vec, we train a neural network to predict the context (neighboring words) of a target word. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given a word, $\\textsf{vegetable}$, and any other word $ w \\in V $, predict the probability that $ w $ occurs in the context of $\\textsf{vegetable}$ (i.e. within a  $\\pm$2 word window):\n",
    "\n",
    "$$P (w | \\textsf{vegetable})$$\n",
    "\n",
    "> <font face=\"verdana\">the carrot is <span style=\"background-color: #9ce59e\"><ins>a root</span><span style=\"background-color: #fcc86f\"> vegetable</span><span style=\"background-color: #9ce59e\">, <ins>usually orange</span></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Wpord2vec\n",
    "\n",
    "During the process we learn embedding representations of the words as a side effect!\n",
    "\n",
    "Te assumption behind the algorithm is that\n",
    "> \"the meaning of a word can be inferred by the company it keeps.\"\n",
    "\n",
    "This approach is unsupervised or __self-supervised__: there's no need for class labels because the (self-)supervision comes from the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word2Vec is a shallow network\n",
    "\n",
    "\n",
    "<img src=\"../images/nlp/single-target-single-context.png\" width=\"900\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word2vec\n",
    "\n",
    "Predict different probablities for different places in the context.\n",
    "\n",
    "<center><img src=\"../images/nlp/skipgram.png\" width=\"400\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word2Vec\n",
    "\n",
    "- Input onehot-encoded target word from vocabulary\n",
    "- Predict the probability of the other words in the vocabulary being a context for the target word\n",
    "\n",
    "<img src=\"../images/nlp/word2vec_onehot.png\" width=\"900\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Look up word vectors and compare them\n",
    "\n",
    "Let $\\mathbf{v}_{w_i}$ be the vector of weights going from input target word $w_I$ and $\\mathbf{v}_{c_j}$ be the vector of weights going to context word $c_j$.\n",
    "\n",
    "Then the probability of a context word $c_j$ given a target $w_I$ can be calculated using a softmax:\n",
    "\n",
    "$p(c_j|w_I) = \\frac{\\exp{( \\mathbf{v}_{c_j}^T \\mathbf{v}_{w_I})}}{\\sum_{k \\in V} \\exp ( \\mathbf{v}_{c_k}^T \\mathbf{v}_{w_I}) }$\n",
    "\n",
    "\n",
    "<img src=\"../images/nlp/word2vec_onehot.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Word2vec\n",
    "\n",
    "The weight vectors Word2vec learns in the process can be used as embeddings!\n",
    "\n",
    "<img src=\"../images/nlp/word2vec_output_weights_function.png\" style=\"width: 70%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# fastText\n",
    "\n",
    "Efficient learning of word representations and sentence classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Similar to Word2vec but it processes words on an n-gram level.\n",
    "\n",
    "For example, the word \"this\" split into the following bi-grams,\n",
    "\n",
    "> \\<t th hi is s>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Simple architecture but its performance for sentiment analysis and tag prediction is on par with complexer models (e.g. char-CRNN) and it's much faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# fastText\n",
    "\n",
    "The intuition behind fastText is that by using a bag of character n-grams, you can learn representations for morphologically rich languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example in plain vanilla Word2Vec, you learn separate representations for \"foot\" and \"football\". This makes it harder to infer that these two words are in fact related.\n",
    "\n",
    "<img src=\"../images/nlp/football.jpg\" align=\"center\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, by learning the character n-gram representation of these words, \"foot\" and \"football\" will now share overlapping n-grams, making them closer in vector space. And thus, would make it easier to surface related concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Advantages\n",
    "\n",
    "- Less features to learn from\n",
    "- Points in space close to each other are similar\n",
    "- Relations captured in dimensions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Embeddings layer\n",
    "\n",
    "- Dense layer that maps one-hot vectors to smaller space\n",
    "- Relations ~ distances\n",
    "- Directions ~ semantic relationships\n",
    "\n",
    "<img src=\"../images/nlp/linear-relationships.png\" width=\"800\" align=\"center\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example document\n",
    "\n",
    "One approach would be to simply sum/average the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_document()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "However, this would lose a lot the sequential information contained in sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNNs are a natural fit for sequences\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Feed-forward 💔 sequences\n",
    "\n",
    "We need a different kind of unit!\n",
    "\n",
    "<img src=\"../images/nlp/feedforward-sequence.png\" style=\"width: 25%; display: block; margin-left: auto !important; margin-right: auto !important;\" align=\"center\"/>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent ❤️ sequences\n",
    "\n",
    "Internal loop feeds back the previous state \n",
    "\n",
    "<img src=\"../images/nlp/rnn-architecture.png\" style=\"width: 80%; display: block; margin-left: auto !important; margin-right: auto !important;\" align=\"center\"/>  \n",
    "\n",
    "Example: understanding a document\n",
    "-  final score $\\mathbf{h}^{(T)}$ represents what the neural network has learned about the document.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example: Document retrieval\n",
    "\n",
    "Architecture to match questions to answers:\n",
    "\n",
    "<img src=\"../images/nlp/rnn-retrieval.png\" style=\"width: 40%;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example: Document retrieval\n",
    "\n",
    "Example: chatbot using an encoder and decoder:\n",
    "\n",
    "<img src=\"../images/nlp/seq2seq-chatbot.png\" style=\"width: 90%;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we exmined how to encode words for use with ML/DL algorithms via Word2vec and fastText.\n",
    "\n",
    "We then discussed how to process these word sequentially using an RNN/LSTM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example & exercise\n",
    "\n",
    "[Sentiment analysis](../exercises/03_03_nlp_sentiment_classification.ipynb)\n",
    "\n",
    "\n",
    "<img src=\"../images/nlp/sentiment-neuron.gif\" style=\"width: 80%; display: block; margin-left: auto !important; margin-right: auto !important;\" align=\"center\"/>  \n",
    "\n",
    "    \n",
    "![footer_logo](../images/logo.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
