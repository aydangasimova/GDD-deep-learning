{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)\n",
    "\n",
    "## Goal\n",
    "\n",
    "We will discuss Long Short Term Memory Units (LSTMs) and how they better enable us to perform sequential tasks.\n",
    "\n",
    "## Program\n",
    "\n",
    "- [Recap: RNNs]()\n",
    "- [The Vanishing/Exploding Gradients problem]()\n",
    "- [LSTMs]()\n",
    "- [GRU]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap: recurrent neural network (RNN)\n",
    "\n",
    "\n",
    "- Proposed in the 80s for modeling time series\n",
    "\n",
    "- An RNN does not start its \"thinking\" from scratch\n",
    "\n",
    "- Networks can persist information\n",
    "\n",
    "- Take earlier inputs into account when making predictions\n",
    "\n",
    "![center three_quarters](../images/rnn/RNN_unrolled.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap: RNN architectures\n",
    "\n",
    "![center half](../images/rnn/rnn_sequence.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recap: backpropagation through time ([BPTT](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf))\n",
    "\n",
    "To train an RNN we backpropogate the error through time.\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\Sigma_t\\frac{\\partial L_t}{\\partial w}$$\n",
    "\n",
    "Backpropagation of the $\\delta$ error vectors through the network.\n",
    "\n",
    "![center third](../images/rnn/bptt_recurrent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanishing gradients\n",
    "\n",
    "> Long range dependencies are lost if there are many steps backwards through the network\n",
    "\n",
    "![center third](../images/lstm/bptt_recurrent.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing gradients: let's backpropagate\n",
    "\n",
    "Let's define:\n",
    "\n",
    "- Input to each recurrent time step $a_t = f(W_r\\cdot a_{t-1}, W_h\\cdot x_t)$. Where $f$ is an activation function.\n",
    "- Output from each time step $\\hat{y}_t = g(W_o\\cdot a_t)$. Where $g$ is some function representing the ouput layer.\n",
    "- The loss value on each time step $L_t = f(y_t, \\hat{y}_t)$. Remember that the total loss in BPTT is defined as $L=\\sum_t L_t$\n",
    "\n",
    "\n",
    "<center><img src=\"../images/lstm/bptt_recurrent.png\" width=\"400\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing gradients: let's backpropagate\n",
    "\n",
    "![right third](../images/lstm/bptt_recurrent.png)\n",
    "\n",
    "- $a_t = f(W_r\\cdot a_{t-1}, W_h\\cdot x_t)$\n",
    "- $\\hat{y}_t = g(W_o\\cdot a_t)$\n",
    "- $L_t = f(y_t, \\hat{y}_t)$\n",
    "\n",
    "*Back propagation through time:*\n",
    "\n",
    "Let's focus on the loss at $t$, $L_t$.\n",
    "\n",
    "For $W_o$ we have,\n",
    "\n",
    "$\\frac{\\partial L_t}{\\partial W_o} = \\frac{\\partial L_t}{\\partial\\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial W_o}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing gradients: let's backpropagate\n",
    "\n",
    "![right third](../images/lstm/bptt_recurrent.png)\n",
    "\n",
    "- $a_t = f(W_r\\cdot a_{t-1}, W_h\\cdot x_t)$\n",
    "- $\\hat{y}_t = g(W_o\\cdot a_t)$\n",
    "- $L_t = f(y_t, \\hat{y}_t)$\n",
    "\n",
    "*Back propagation through time:*\n",
    "\n",
    "The weight $W_r$ is shared across all the time sequence. Therefore, we can differentiate to it at the each time step and sum all together:\n",
    "\n",
    "$\\frac{\\partial L_t}{\\partial W_r} = \\sum_{n=0}^t\\frac{\\partial L_t}{\\partial\\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial a_t}\\frac{\\partial a_t}{\\partial a_n}\\frac{\\partial a_n}{\\partial W_r}=\\frac{\\partial L_t}{\\partial\\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial a_t}\\sum_{n=0}^t\\frac{\\partial a_t}{\\partial a_n}\\frac{\\partial a_n}{\\partial W_r}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing gradients: let's backpropagate\n",
    "\n",
    "![right third](../images/bptt_recurrent.png)\n",
    "\n",
    "- $a_t = f(W_r\\cdot a_{t-1}, W_h\\cdot x_t)$\n",
    "- $\\hat{y}_t = g(W_o\\cdot a_t)$\n",
    "- $L_t = f(y_t, \\hat{y}_t)$\n",
    "\n",
    "*Back propagation through time:*\n",
    "\n",
    "Using the chain rule and that $u_t = W_r \\cdot a_t$, \n",
    "\n",
    "$\\frac{\\partial a_t}{\\partial a_n}=\\prod_{m=n+1}^t\\frac{\\partial a_m}{\\partial a_{m-1}}=\\prod_{m=n+1}^t\\frac{\\partial a_m}{\\partial u_{m-1}}\\frac{\\partial u_{m-1}}{\\partial W_r} =\\prod_{m=n+1}^t\\frac{\\partial a_m}{\\partial u_{m-1}}\\cdot W_r$ \n",
    "\n",
    "So,\n",
    "\n",
    "$\\frac{\\partial a_t}{\\partial a_n}=W_r^{t - n}\\prod_{m=n+1}^t f'(x)\\bigg\\rvert_{x=u_{m-1}}$ and therefore, $\\frac{\\partial L_t}{\\partial W_r} = \\frac{\\partial L_t}{\\partial\\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial a_t}\\sum_{n=0}^tW_r^{t - n}\\frac{\\partial a_n}{\\partial W_r}\\prod_{m=n+1}^t f'(x)\\bigg\\rvert_{x=u_{m-1}}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing gradients: let's backpropagate\n",
    "\n",
    "$$\\frac{\\partial L_t}{\\partial W_r} = \\frac{\\partial L_t}{\\partial\\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial a_t}\\sum_{n=0}^tW_r^{t - n}\\frac{\\partial a_n}{\\partial W_r}\\prod_{m=n+1}^t f'(x)\\bigg\\rvert_{x=u_{m-1}}$$\n",
    "\n",
    "Contribution to the loss coming from first observation $n=0$:\n",
    "\n",
    "$$\\frac{\\partial L_t}{\\partial W_r} = \\Big(\\color{red}{W_r^t}\\cdot\\prod_{m=1}^t f'(x)\\bigg\\rvert_{x=u_{m-1}}\\Big)\\cdot\\frac{\\partial L_t}{\\partial\\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial a_t}\\frac{\\partial a_0}{\\partial W_r}$$\n",
    "\n",
    "- The term $\\color{red}{W_r^t}$ can cause exploding and vanishing gradient problems. The contribution from observations explodes or decays exponentially as inputs are farther from the output ($t >> n$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.01**100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.1**100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing gradients: let's backpropagate\n",
    "\n",
    "Contribution coming from first observation $n=0$:\n",
    "$$\\frac{\\partial L_t}{\\partial W_r} = \\Big(W_r^t\\cdot\\color{red}{\\prod_{m=1}^t f'(x)}\\bigg\\rvert_{x=u_{m-1}}\\Big)\\cdot\\frac{\\partial L_t}{\\partial\\hat{y}_t}\\frac{\\partial\\hat{y}_t}{\\partial a_t}\\frac{\\partial a_0}{\\partial W_r}$$\n",
    "\n",
    "- The term $\\color{red}{\\prod_{m=1}^t f'(x)}$ can cause vanishing gradients problems, since $f$ is an activation function\n",
    "\n",
    "![center third](../images/lstm/vanishing_gradient_b.png)\n",
    "\n",
    "<sup>See this [tutorial](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/) for a more details regarding the BPTT methodology.</sup>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Vanishing & exploding gradients\n",
    "\n",
    "Long range dependencies -> many steps backwards through the network -> vanishing/exploding gradients\n",
    "\n",
    "This gradient issue with RNNs can be combatted by\n",
    "- Weight regularization (exploding gradients due to $\\color{red}{W_r^t}$)\n",
    "- Weight clipping (vanishing/exploding gradients due to $\\color{red}{W_r^t}$)\n",
    "- ReLU activation functions (vanishing gradients due to $\\color{red}{\\prod_{m=1}^t f'(x)}$) \n",
    "\n",
    "However, alternative network architectures also address this issue.\n",
    "- Long short term memory (LSTM)\n",
    "- Gated recurrent unit (GRU)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Long short term memory unit (LSTM)\n",
    "\n",
    "- The concept of LSTMs was [first proposed in 1997](http://www.bioinf.jku.at/publications/older/2604.pdf) and nowadays they are frequently used in NLP.\n",
    "- Popular approach to dealing with the vanishing gradient problem\n",
    "- Efficiently learning long-range dependencies in sequences. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM \n",
    "\n",
    "![center three_quarters](../images/lstm/LSTM_overview.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### LSTM cell memory\n",
    "\n",
    "At the core of the LSTM cell is a state vector $\\boldsymbol{C}_t$, it is the key to cell's 'memory'. In the figure below the continuity of the cell state vector is represented by the horizontal line running through the top of the diagram.\n",
    "\n",
    "\n",
    "<center><img src=\"../images/lstm/LSTM_cell_state.png\" width=\"500\"><center>\n",
    "\n",
    "Notice that the cell state runs straight down the entire chain, with only some minor linear interactions. As a result, it is very easy for information flow along it. This mechanism provides the network with the long term memory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Forgetting\n",
    "\n",
    "The decision of what information to forget is made by the sigmoid 'forget gate layer', as shown in the figure below. This layer looks at the previous hidden state $\\boldsymbol{h}_{t-1}$ and the current input $\\boldsymbol{x}_t$. It outputs a vector $\\boldsymbol{f}_t$ with numbers between $0$ and $1$, where $0$ represents *completely forgetting* the cell state value while $1$ represents *keep remembering* it.\n",
    "\n",
    "![center half](../images/lstm/LSTM_forgetting.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Storing\n",
    "\n",
    "The decision of what new information to store is made by the interaction between the sigmoid 'input gate layer' and the tanh 'candidate state layer', as shown in the figure below. Both layers again look at the previous hidden state and the current input. The output vector of the input gate layer $\\boldsymbol{i}_t$ modulates the candidate cell state $\\tilde{\\boldsymbol{C}}_t$ which is computed by the candidate state layer.\n",
    "\n",
    "![center half](../images/lstm/LSTM_storing.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Updating\n",
    "\n",
    "Finally, the new cell state vector $\\boldsymbol{C}_t$ is updated by applying the forget operation to the previous cell state and combining that with the modulated new candidate state. This process is detailed in the figure below.\n",
    "\n",
    "![center three_quarters](../images/lstm/LSTM_updating.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Generating output\n",
    "\n",
    "Once the cell state vector has been computed the LSTM is ready to generate its output. The decision of what information to output is made by the sigmoid 'output gate layer', as shown in the figure below. Like the other gate layers, this also layer looks at previous hidden state and the current input and outputs a vector $\\boldsymbol{o}_t$ with numbers between $0$ and $1$. The cell state is put through a $\\tanh$ function to push its values between $-1$ and $1$, it is then modulated by the output gate to created the $\\boldsymbol{h}_t$ hidden state vector.\n",
    "\n",
    "![center three_quarters](../images/lstm/LSTM_output.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Question - Why \"long short-term memory\"?\n",
    "Knowing how LSTMs work can you explain their name?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "[LSTM paper](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    ">\"Recurrent networks can in principle use their feedback connections to store representations of recent input events in the form of activations (\"short-term memory\", as opposed to \"long-term memory embodied by slowly changing weights)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GRU, an alternative to LSTM\n",
    "\n",
    "* The Gated Recurrent Unit is very similar to LSTM\n",
    "* Similar optimization performance, however less computation overhead\n",
    "* Notice how $z_i(t) \\in [0, 1]$, being sigmoid selects the previous state, $h_i(t-1)$, or the candidate state $\\hat{h}_i(t)$\n",
    "* For more details, comparing LSTM and GRU, consult [this article](https://arxiv.org/abs/1412.3555)\n",
    "\n",
    "![center half](../images/lstm/gru.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GRU details\n",
    "\n",
    "* When $z_i(t) = 0$:\n",
    "    * Then $1 - z_i(t) = 1$, multiplication node does not affect the previous state: $h_i(t-1)*(1-z_i(t))$\n",
    "    * Then $z_i(t) \\hat{h}_i(t) = 0$. Addition node keeps hidden state unaffected\n",
    "* When $z_i(t) = 1$:\n",
    "    * Then $1- z_i(t) = 0$, mutliplication node clears the previous state\n",
    "    * Then $z_i(t) \\hat{h}_i(t) = \\hat{h}_i(t)$. The addition node accepts the new state on the cleared state\n",
    "        \n",
    "        \n",
    "![center half](../images/lstm/gru.png)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## GRU vs. LSTM\n",
    "\n",
    "- The key difference between GRU and LSTM is that GRU's bag has two gates that are reset and update while LSTM has three gates that are input, output, forget. GRU is less complex than LSTM because it has less number of gates.\n",
    "\n",
    "- If the dataset is small then GRU is preferred otherwise LSTM for the larger dataset.\n",
    "\n",
    "- GRU exposes the complete memory and hidden layers but LSTM doesn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "After reviewing the RNN we discussed its crucial flaw: the vanishing/exploding graident problem.\n",
    "\n",
    "We then discussed solutions in the form of an LSTM and a GRU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# LSTM Exercise\n",
    "[Exercise: Human activity recognition](../exercises/03_02_lstm_human_activity_recognition.ipynb)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
