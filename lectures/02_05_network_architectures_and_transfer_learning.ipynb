{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-dancing",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-captain",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Network Architectures & Transfer Learning\n",
    "\n",
    "![footer_logo](../images/logo.png)\n",
    "\n",
    "\n",
    "## Goal\n",
    "\n",
    "We will discuss some of the most important and popular **deep learning architectures** for CNNs. Afterwards, we discuss the concept of **transfer learning**.\n",
    "\n",
    "## Program\n",
    "\n",
    "Benchmarking dataset\n",
    "- [ImageNet]()\n",
    "\n",
    "Famous networks\n",
    "- [AlexNet]()\n",
    "- [VGG]()\n",
    "- [Inception]()\n",
    "- [ResNet]()\n",
    "\n",
    "Transfer Learning\n",
    "- [Transfer Learning]()\n",
    "    - [Fine-tuning]()\n",
    "    - [Feature extraction]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-pilot",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# ImageNet\n",
    "\n",
    "The full datasets consists of 14 million images, 20,000 categories\n",
    "\n",
    "![](../images/network_architechture/imagenet.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-windsor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ImageNet\n",
    "\n",
    "\n",
    "A bench mark for image classification\n",
    "\n",
    "![](../images/network_architechture/benchmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-choir",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AlexNet\n",
    "\n",
    "<center><img src=\"../images/network_architechture/alexnet.png\" width=\"800\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-lithuania",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# [AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "- Publisehd by Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton in 2014\n",
    "- One of the first fast GPU-implementations of a CNN to win an image recognition contest.\n",
    "- Considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning.\n",
    "- As of 2020, the AlexNet paper has been cited over 70,000 times according to Google Scholar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-graduation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# AlexNet\n",
    "\n",
    "18.2% top-5 error rate on ImageNet\n",
    "\n",
    "<center><img src=\"../images/network_architechture/alexnet_drawing.png\" width=\"1000\"><center>\n",
    "\n",
    "[Gavves, E. (2019)](https://uvadlc.github.io/lectures/apr2019/lecture4-convnets.pdf)\n",
    "\n",
    "*Top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-competition",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Removing layers\n",
    "\n",
    "- layer 7: 16 million less paramters, 1.1% drop in performance\n",
    "- layer 6 & 7: 50 million less paramters, 5.7% drop in performance\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/alexnet_drawing.png\" width=\"1000\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-tribute",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Removing layers: convolutions are important\n",
    "\n",
    "- layer 7: 16 million less paramters, 1.1% drop in performance\n",
    "- layer 6 & 7: 50 million less paramters, 5.7% drop in performance\n",
    "- layer 3 & 4: 1 million less paramters, 3% drop in performance\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/alexnet_drawing.png\" width=\"1000\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-grave",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Removing layers: depth is important!\n",
    "\n",
    "Removing layers 3,4,6 & 7 results in a 33.5% drop in performance!\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/alexnet_drawing.png\" width=\"1000\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-camera",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [VGG-16](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "\n",
    "- 7.3% error rate in ImageNet (compared to 18.2% with AlexNet)\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/vgg16.png\" width=\"400\"><center>\n",
    "\n",
    "[image source](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-layer",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3x3 Convolutions\n",
    "\n",
    "- The smallest possible filter to captures the “up”, “down”, “left”, “right”, \"center\" of a region.\n",
    "- Two back to back 3x3 convolutions have the effective receptive field of a single 5x5 convolution. Here’s the visualization of two stacked 3x3 convolutions resulting in 5x5.\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/convolutions_55_33.png\" width=\"500\"><center>\n",
    "\n",
    "[image source](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)\n",
    "\n",
    "- 1 large filter can be replaced by a deeper stack of successive smaller filters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-nigeria",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Inception](https://arxiv.org/abs/1409.4842v1)\n",
    "\n",
    "- 6.67% error rate in ImageNet (compared to 18.2% with AlexNet)\n",
    "- GoogLeNet (Version 1) has 22 layers\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/inception.png\" width=\"1000\"><center>\n",
    "\n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca22c1b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Motivation behind network architecture**:\n",
    "\n",
    "Salient parts have great variation in sizes. Hence, the receptive fields should vary in size accordingly.\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/dogs.jpeg\" width=\"800\"></center>\n",
    "    \n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-animation",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Also, \n",
    "- Naively stacking convolutional operations is expensive\n",
    "- Very deep nets are prone to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1410f76",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Naive Solution**:\n",
    "\n",
    "Multiple kernel filters of different sizes (1 × 1, 3 × 3, 5 × 5)\n",
    "\n",
    "\n",
    "<center><img src=\"../images/network_architechture/inception_module.png\" width=\"700\"></center>\n",
    "\n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-yesterday",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Still computationally expensive!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-czech",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Better Solution**:\n",
    "\n",
    "Before applying convolutions, combine the input channels with 1x1 convolutions.\n",
    "\n",
    "<center><img src=\"../images/network_architechture/inception_module.png\" width=\"700\"></center>\n",
    "\n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-stupid",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [ResNet](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<center><img src=\"../images/network_architechture/resnet.jpeg\" width=\"1000\"></center>\n",
    "\n",
    "\n",
    "[image source](https://www.kaggle.com/keras/resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-coating",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "- The first truly Deep Network, going deeper than 1,000 layers\n",
    "- More importantly, the first Deep Architecture that proposed a novel concept on how to gracefully go deeper than a few dozen layers (Not simply getting more GPUs, more training time, etc.)\n",
    "- Smashed Imagenet, with a 3.57% error \n",
    "- Won a variety of challenges: object classification, detection, segmentation, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-questionnaire",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation: Going deeper had its limits\n",
    "\n",
    "<center><img src=\"../images/network_architechture/toodeep.png\" width=\"900\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-scholarship",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the problem?\n",
    "\n",
    "- Very deep networks stop learning after a bit\n",
    "- An accuracy is reached, then the network saturates and starts unlearning\n",
    "- Signal gets lost through so many layers\n",
    "\n",
    "<center><img src=\"../images/network_architechture/toodeep.png\" width=\"900\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-latvia",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution: The residual block\n",
    "\n",
    "Reinserting the original image at different stages of the network means we can have deeper networks\n",
    "\n",
    "<center><img src=\"../images/network_architechture/residual_connection.png\" width=\"700\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-announcement",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Residual connections\n",
    "\n",
    "Without residual connections deeper networks are untrainable\n",
    "\n",
    "<center><img src=\"../images/network_architechture/deep_residual.png\" width=\"900\"></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-crystal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8296c50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Assume we have two datasets, A and B:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9132773",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Dataset A** has plenty of images and we have already been able to train an accurate model on it ( e.g. think of a ResNet model trained on the ImageNet* dataset).\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ac8e20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Dataset B** has much fewer images. We would struggle to learn an accurate model with this data alone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182eb1a1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can use the the model learnt on dataset A to learn a better model on dataset B!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17bd15",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Even if the image classes of B do not (necessarilly) overlap with A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-champagne",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This is called transfer learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-piano",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use transfer learning\n",
    "\n",
    "The most powerful CNNs have millions of parameters... but our datasets are not always as large. This can result in overfittting, but transfer learning can help us to avoid this!\n",
    "\n",
    "There are two main approaches to transfer learning\n",
    "1. Fine-tuning\n",
    "2. Feature extraction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-starter",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Fine-tuning\n",
    "\n",
    "When fine-tuning, we assume the parameters of the pre-trained model are already close to the optimum for the new dataset.\n",
    "\n",
    "We use the weights as a starting point for the parameters of the new model and fine-tune from there.\n",
    "\n",
    "Best used when the new dataset B is relatively big *e.g. a dataset with more than a few thousand images*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-andorra",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Feature extraction\n",
    "\n",
    "This is similar to fine-tuning, but we train only the loss layer.\n",
    "\n",
    "Essentially use the network as a pretrained feature extractor.\n",
    "\n",
    "Best used when:\n",
    "- The target dataset 𝑇 is small and any fine-tuning of layer might cause overfitting,\n",
    "- Or when we don’t have the resources to train a deep net,\n",
    "- Or when we don’t care for the best possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-theater",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## It is also possible to do something in between, e.g. fine-tune the last few layers!\n",
    "\n",
    "![](../../images/finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-clothing",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transfer Learning is the norm!\n",
    "\n",
    "## Not the exception!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f79eae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we have covered,\n",
    "- ImageNet (a famous dataset for training models and benchmarking performance)\n",
    "- The architechture of famous networks\n",
    "    - [AlexNet]()\n",
    "    - [VGG]()\n",
    "    - [Inception]()\n",
    "    - [ResNet]()\n",
    "- Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131ec8e3",
   "metadata": {},
   "source": [
    "# Transfer Learning: Exercise\n",
    "[Exercise: Keras advanced](../exercises/02_03_transfer_learning.ipynb)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
