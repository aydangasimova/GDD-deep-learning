{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-dancing",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greatest-captain",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Network Architectures & Transfer Learning\n",
    "\n",
    "After first discussing a famous dataset used for benchmarking networks (**ImageNet**), \n",
    "\n",
    "we will discuss some of the most important and popular **deep learning architectures** for CNNs:\n",
    "\n",
    "- AlexNet\n",
    "- VGG\n",
    "- Inception\n",
    "- ResNet\n",
    "\n",
    "We will then discuss the concept of **transfer learning**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-pilot",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ImageNet\n",
    "\n",
    "14 million images, 20,000 categories\n",
    "\n",
    "![](https://miro.medium.com/max/750/1*IlzW43-NtJrwqtt5Xy3ISA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valuable-windsor",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ImageNet\n",
    "\n",
    "\n",
    "A bench mark for image classification\n",
    "\n",
    "![](images/transfer/benchmarks.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-choir",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AlexNet\n",
    "\n",
    "![](https://paperswithcode.com/media/methods/Screen_Shot_2020-06-22_at_6.35.45_PM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-lithuania",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## [AlexNet](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)\n",
    "\n",
    "- Publisehd by Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton in 2014\n",
    "- One of the first fast GPU-implementations of a CNN to win an image recognition contest.\n",
    "- Considered one of the most influential papers published in computer vision, having spurred many more papers published employing CNNs and GPUs to accelerate deep learning.\n",
    "- As of 2020, the AlexNet paper has been cited over 70,000 times according to Google Scholar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-graduation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## AlexNet\n",
    "\n",
    "18.2% top-5 error rate on ImageNet\n",
    "\n",
    "![](images/transfer/alexnet.png)\n",
    "\n",
    "[Gavves, E. (2019)](https://uvadlc.github.io/lectures/apr2019/lecture4-convnets.pdf)\n",
    "\n",
    "*Top-5 error rate is the fraction of test images for which the correct label is not among the five labels considered most probable by the model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demographic-competition",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Removing layers\n",
    "\n",
    "- layer 7: 16 million less paramters, 1.1% drop in performance\n",
    "- layer 6 & 7: 50 million less paramters, 5.7% drop in performance\n",
    "\n",
    "![](images/transfer/alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eastern-tribute",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Removing layers: convolutions are important\n",
    "\n",
    "- layer 7: 16 million less paramters, 1.1% drop in performance\n",
    "- layer 6 & 7: 50 million less paramters, 5.7% drop in performance\n",
    "- layer 3 & 4: 1 million less paramters, 3% drop in performance\n",
    "\n",
    "![](images/transfer/alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-grave",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Removing layers: depth is important!\n",
    "\n",
    "Removing layers 3,4,6 & 7 results in a 33.5% drop in performance!\n",
    "\n",
    "![](images/transfer/alexnet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-camera",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [VGG-16](https://arxiv.org/pdf/1409.1556.pdf)\n",
    "\n",
    "- 7.3% error rate in ImageNet (compared to 18.2% with AlexNet)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*U8uoGoZDs8nwzQE3tOhfkw@2x.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "[image source](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-layer",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3x3 Convolutions\n",
    "\n",
    "- The smallest possible filter to captures the ‚Äúup‚Äù, ‚Äúdown‚Äù, ‚Äúleft‚Äù, ‚Äúright‚Äù, \"center\".\n",
    "- Two back to back 3x3 convolutions have the effective receptive field of a single 5x5 convolution. Here‚Äôs the visualization of two stacked 3x3 convolutions resulting in 5x5.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*YpXrr8bN5XyqOlztKPHvDw@2x.png\" width=\"400\" align=\"center\"/>\n",
    "\n",
    "[image source](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)\n",
    "\n",
    "- 1 large filter can be replaced by a deeper stack of successive smaller filters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blessed-nigeria",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [Inception](https://arxiv.org/abs/1409.4842v1)\n",
    "\n",
    "- 6.67% error rate in ImageNet (compared to 18.2% with AlexNet)\n",
    "- GoogLeNet (Version 1) has 22 layers\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*uW81y16b-ptBDV8SIT1beQ.png)\n",
    "\n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-animation",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Motivation behind network architecture**:\n",
    "\n",
    "Salient parts have great variation in sizes. Hence, the receptive fields should vary in size accordingly.\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*aBdPBGAeta-_AM4aEyqeTQ.jpeg)\n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n",
    "\n",
    "Also, \n",
    "- Naively stacking convolutional operations is expensive\n",
    "- Very deep nets are prone to overfitting.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-yesterday",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Naive Solution**:\n",
    "\n",
    "Multiple kernel filters of different sizes (1 √ó 1, 3 √ó 3, 5 √ó 5)\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*DKjGRDd_lJeUfVlY50ojOA.png)\n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n",
    "\n",
    "Still computationally expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-czech",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Better Solution**:\n",
    "\n",
    "Combine channels with 1x1 convolutions\n",
    "[image source](https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202)\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*U_McJnp7Fnif-lw9iIC5Bw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-stupid",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# [ResNet](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1400/1*S3TlG0XpQZSIpoDIUCQ0RQ.jpeg\" style=\"width:700px\"/>\n",
    "\n",
    "[image source](https://www.kaggle.com/keras/resnet50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-coating",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ResNet\n",
    "\n",
    "- The first truly Deep Network, going deeper than 1,000 layers\n",
    "- More importantly, the first Deep Architecture that proposed a novel concept on how to gracefully go deeper than a few dozen layers (Not simply getting more GPUs, more training time, etc.)\n",
    "- Smashed Imagenet, with a 3.57% error \n",
    "- Won a variety of challenges: object classification, detection, segmentation, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-questionnaire",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Motivation: Going deeper had its limits\n",
    "\n",
    "\n",
    "![](https://miro.medium.com/max/770/0*Um4XSkHDcmLyeeiy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appointed-scholarship",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is the problem?\n",
    "\n",
    "- Very deep networks stop learning after a bit\n",
    "- An accuracy is reached, then the network saturates and starts unlearning\n",
    "- Signal gets lost through so many layers\n",
    "\n",
    "![](https://miro.medium.com/max/770/0*Um4XSkHDcmLyeeiy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-latvia",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Solution: The residual block\n",
    "\n",
    "Reinserting the original image at different stages of the network means we can have deeper networks\n",
    "\n",
    "![](https://miro.medium.com/max/868/0*sGlmENAXIZhSqyFZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "threatened-announcement",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Residual connections\n",
    "\n",
    "Without residual connections deeper networks are untrainable\n",
    "\n",
    "![](https://miro.medium.com/max/1400/0*AMK5ylLHQQ3CLQzk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fossil-crystal",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neither-champagne",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transfer Learning: motivation\n",
    "\n",
    "Assume we have two datasets, A and B\n",
    "- Dataset A is \n",
    "    - fully annotated, plenty of images\n",
    "    - We already have an accurate model for this dataset *e.g. a ResNet trained on ImageNet*\n",
    "    \n",
    "- Dataset B is\n",
    "    - Not as much annotated, or much fewer images\n",
    "    - The annotations of B do not (necessarilly) overlap with A\n",
    "\n",
    "We can use the the model learnt on dataset A to learn a better model on dataset B!\n",
    "\n",
    "This is called transfer learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gothic-piano",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why use transfer learning\n",
    "\n",
    "The most powerful CNNs have millions of parameters... but our datasets are not always as large. This can result in overfittting, but transfer learning can help us to avoid this!\n",
    "\n",
    "There are two main approaches to transfer learning\n",
    "1. Fine-tuning\n",
    "2. Feature extraction\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-starter",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1. Fine-tuning\n",
    "\n",
    "When fine-tuning, we assume the parameters of the pre-trained model are already close to the optimum for the new dataset.\n",
    "\n",
    "We use the weights as a starting point for the parameters of the new model and fine-tune from there.\n",
    "\n",
    "Best used when the new dataset B is relatively big *e.g. a dataset with more than a few thousand images*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-andorra",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2. Feature extraction\n",
    "\n",
    "This is similar to fine-tuning, but we train only the loss layer.\n",
    "\n",
    "Essentially use the network as a pretrained feature extractor.\n",
    "\n",
    "Best used when:\n",
    "- The target dataset ùëá is small and any fine-tuning of layer might cause overfitting,\n",
    "- Or when we don‚Äôt have the resources to train a deep net,\n",
    "- Or when we don‚Äôt care for the best possible accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-theater",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## It is also possible to do something in between, e.g. fine-tune the last few layers!\n",
    "\n",
    "![](images/transfer/finetuning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-clothing",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning is the norm!\n",
    "\n",
    "# Not the exception!\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
