{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: intuition\n",
    "\n",
    "![neuron_comparison center half](../images/neuron_comparison.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks: intuition\n",
    "\n",
    "![neuron_comparison center half](../images/neuron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "* **Neuron (node)**: element of a network where inputs (vector **x**) are combined with weights (vector **w**), a bias (value *b*) and a non-linear activation function (σ) to produce an output value, \n",
    "i.e. $\\text{output = σ}(w^T x + b)$ \n",
    "* **Layer**: a column of neurons stacked together that can receive the same inputs.\n",
    "* **Hidden layers**: intermediate layers between inputs and outputs.\n",
    "* **Deep neural network**: a neural network that contains many hidden layers, and can therefore provide solutions to more complicated and subtle decision problems. \n",
    "\n",
    "\n",
    "![neuron_comparison center half](../images/neuron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks: training\n",
    "\n",
    "\n",
    "*Training*: the process of tuning the weights **w** in a network by providing the network with example data; a combination of input data **X** and the target label the network should predict *y*.  \n",
    "\n",
    "* Gradient descent: process of following the gradients of the error function towards a minimum value\n",
    "* Backpropagation: fast algorithm for computing such gradients based on the chain-rule\n",
    "\n",
    "\n",
    "\n",
    "![center](../images/cost_function_gradient.png)\n",
    "\n",
    "### <center> $ w' = w - \\eta \\frac{\\partial J(w)}{\\partial w }$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks: training\n",
    "\n",
    "Epoch: \n",
    "\n",
    "1. *Forward pass*: a data sample is passed forward through the network to determine a prediction\n",
    "2. *Backward pass*: recursively compute the error backwards from the last layer following the chain-rule and update the weights w.r.t. the known target output. \n",
    "\n",
    "Requirement: all elements of the neural network should be differentiable\n",
    "\n",
    "\n",
    "![center](../images/model_diagram.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "### > Feed data through network\n",
    "\n",
    "![center](../images/forward_pass_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "### > Feed data through network\n",
    "\n",
    "\n",
    "![center](../images/forward_pass_1.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "### > Error = truth - output\n",
    "(or error function)\n",
    "\n",
    "![center](../images/forward_pass_2.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Local error contribution\n",
    "![center](../images/backpropagation_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Local error contribution\n",
    "\n",
    "\n",
    "![center](../images/backpropagation_1.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "### > We want to find the best weights!\n",
    "\n",
    "#### Update weights using gradient descent\n",
    "\n",
    "#### $ w'_{(x_1)1} = w_{(x_1)1} - \\eta \\frac{\\partial \\text{loss}}{\\partial w_{(x_1)1} }$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "#### we find $\\frac{\\partial \\text{loss}}{\\partial w_{(x_1)1} }$ using the chain rule\n",
    "\n",
    "#### $ \\frac{\\partial \\text{loss}}{\\partial w_{(x_1)1}} = \\frac{\\partial \\text{loss}}{\\partial f_1(e)} \\frac{\\partial f_1(e)}{\\partial e} \\frac{\\partial e}{\\partial w_{(x_1)1}} $, where $e=w_{(x_1)1}x_1 + w_{(x_2)1}x_2 $\n",
    "\n",
    "\n",
    "![center](../images/forward_pass_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "### > We want to find the best weights!\n",
    "\n",
    "#### Update weights using gradient descent\n",
    "\n",
    "#### $ w'_{(x_1)1} = w_{(x_1)1} - \\eta \\frac{\\partial \\text{loss}}{\\partial w_{(x_1)1} }$\n",
    "\n",
    "#### we find $\\frac{\\partial \\text{loss}}{\\partial w_{(x_1)1} }$ using the chain rule\n",
    "\n",
    "#### $ \\frac{\\partial \\text{loss}}{\\partial w_{(x_1)1}} = \\frac{\\partial \\text{loss}}{\\partial f_1(e)} \\frac{\\partial f_1(e)}{\\partial e} \\frac{\\partial e}{\\partial w_{(x_1)1}} $, where $e=w_{(x_1)1}x_1 + w_{(x_2)1}x_2 $\n",
    "\n",
    "Thus,\n",
    "\n",
    "$ \\frac{\\partial \\delta}{\\partial w_{(x_1)1}} = -\\delta_1 \\frac{\\partial f_1(e)}{\\partial e} x_1 $ \n",
    "![center](../images/forward_pass_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Optimize weight with local error\n",
    "\n",
    "\n",
    "![center](../images/backpropagation_2.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Optimize weight with local error\n",
    "\n",
    "\n",
    "![center](../images/backpropagation_3.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion \n",
    "\n",
    "* Intuition to neural networks: perceptron\n",
    "* Backpropagation\n",
    "\n",
    "### [Exercise: gradient descent for XOR perceptron](../exercises/01-01_exercises_xor_perceptron.ipynb)\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
