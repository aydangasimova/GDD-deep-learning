{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural networks in practice\n",
    "- What type of learning (machine learning vs. deep learning) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What type of neural network? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- What tooling do I use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do I get the best performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- How do I monitor my performance? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Type of learning\n",
    "\n",
    "* Simple heuristics\n",
    "* Machine learning\n",
    "* Deep learning / neural networks\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep  learning or Machine learning?\n",
    "\n",
    "### Data type\n",
    "* **Structured**: tabular data\n",
    "    - Handcrafted feature engineering\n",
    "    - Boosting algoritms    \n",
    "* **Unstructured**: images/text/signals\n",
    "    - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Amount of data\n",
    "\n",
    "To cite the [Deep Learning](http://www.deeplearningbook.org/contents/intro.html) book:\n",
    "\n",
    ">  As of 2016, a rough rule of thumb is that a supervised deep learning algorithm will generally achieve acceptable performance with around **5,000 labeled examples per category**, and will match or exceed human performance when trained with a dataset containing at least 10 million labeled examples. Working successfully with datasets smaller than this is an important research area, focusing in particular on how we can take advantage of large quantities of unlabeled examples, with **unsupervised or semi-supervised learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Problem Complexity\n",
    "<center><img src=\"../images/chihuahua-muffin.png\" width=\"700\"><center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question\n",
    "### Deep learning or machine learning? \n",
    "> You want to predict the expected stay of a patient in the ICU based on their medical history contained in the electronic health records, and the lab test and measurements results during their stay so far in the ICU such as heart rate, blood pressure, etc. Do you use deep learning or traditional machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Question\n",
    "### Deep learning or machine learning? \n",
    "\n",
    "> You want to create a model that acts as a second reader for radiologists - the doctors that read medical images. It helps the radiologist regions of interest that might be early sign of lung cancer on (3D) CT scans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Question \n",
    "### Deep learning or machine learning? \n",
    "> You want to create a system that generates a quick overview of the sentiment of a number of product reviews on a website. Deep learning or traditional machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What type of neural network? \n",
    "* Fully-connected neural networks\n",
    "* Convolutional neural networks\n",
    "* Recurrent neural networks\n",
    "* Generative neural networks\n",
    "\n",
    "<center><img src=\"../images/neural_networks_collection.png\" width=\"500\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully-connected neural networks\n",
    "**Use it for**: \n",
    "- Tabular datasets\n",
    "\n",
    "![simple nn](../images/model_diagram.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Convolutional Neural Networks\n",
    "* Neural networks with convolutional (and pooling) layers\n",
    "* Typically work well on data with a spatial relationship\n",
    "* Translation invariant\n",
    "![](../images/ender-translated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Convolutional Neural Networks\n",
    "\n",
    "**Use it for**: images\n",
    "\n",
    "**Try it on**: text data, time series data, sequence input data\n",
    "\n",
    "![](../images/example_conv_net.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Recurrent Neural Networks\n",
    "\n",
    "* Sequence prediction problems.\n",
    "* LSTM is the most successful RNNs\n",
    "\n",
    "**Use it for**: text data, speech data\n",
    "\n",
    "**Don't use it for**: tabular data, image data\n",
    "\n",
    "<center><img src=\"../images/rnn-architecture.png\" width=\"400\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#  Generative Models\n",
    "\n",
    "* Generative-Adversarial Networks (GANs)\n",
    "* Variational-Autoencoders\n",
    "\n",
    "![](https://miro.medium.com/max/1400/1*BaZPg3SRgZGVigguQCmirA.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question\n",
    "## What type of neural network? \n",
    "> You have a dataset of images of objects. For each object, you have 72 different viewpoints. This is your training set. Your goal is that, given an image of an object, you want to output an image of that object from a different viewpoint. What type of network would you use?\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/profile/Jean_Elsner/publication/329969195/figure/fig11/AS:708799614705664@1546002405051/Samples-of-different-viewpoints-from-the-object-recognition-dataset-The-same-object.ppm\" width=\"400\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question\n",
    "## What type of neural network? \n",
    "> You have a collection of audio fragments from music. You want to determine the genre the fragment belongs to. What type of network do you use? \n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/5/57/Treble_a.svg/1200px-Treble_a.svg.png\" width=\"200\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question\n",
    "## What type of neural network? \n",
    "> You have a collection of ECGs from patients, which denote the heart rhythm over time. You want to distinguish between normal heart rhythm cases and atrial fibrillation (abnormal). What type of neural network would be most suitable for this problem?\n",
    "\n",
    "<center><img src=\"../images/ecg.png\" width=\"200\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Infrastructure\n",
    "\n",
    "* Local, server or cloud?\n",
    "* Cloud providers: \n",
    "    - Amazon Web Services (AWS)\n",
    "    - Google Cloud Platform (GCP)\n",
    "    - Microsoft Azure\n",
    "\n",
    "![](../images/cloud-logos.png)\n",
    "   \n",
    "   \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Infrastructure\n",
    "\n",
    "### CPU vs GPU \n",
    "    \n",
    "Required for GPU: \n",
    "* CUDA: API for Parallel Computing by NVIDIA\n",
    "* cuDNN: GPU-accelerated library of primitives for deep neural networks\n",
    "    \n",
    "![Nvidia cards center third](../images/nvida_cards_vs.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimize model performance\n",
    "\n",
    "How do I get the best model performance?\n",
    "* How do I preprocess my data? \n",
    "* What hyperparameters & architecture?\n",
    "* Regularization\n",
    "* From scratch or transfer learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data preprocessing\n",
    "\n",
    "#### Feature data\n",
    "- Zero-center: subtract the mean from every data point\n",
    "- Standardize/scale: divide by the standard deviation\n",
    "\n",
    "![center third](../images/feature_scaling.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question\n",
    "## Why do we normalize the inputs x?\n",
    "\n",
    "a) It makes the parameter initialization faster\n",
    "\n",
    "b) Normalization is another word for regularization--It helps to reduce variance\n",
    "\n",
    "c) It makes the cost function faster to optimize\n",
    "\n",
    "d) It makes it easier to visualize the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "&rarr; c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data preprocessing\n",
    "\n",
    "#### Target data\n",
    "- Multi-label classification: one-hot encode the categorical targets\n",
    "- Regression: match target range with output activation function\n",
    "\n",
    "![center half](../images/target_one_hot.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameter choices \n",
    "\n",
    "- Gradient descent\n",
    "- Loss function\n",
    "- Weight updates\n",
    "- Weight initialization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "Mini-batch gradient descent is the best choice in most cases. \n",
    "\n",
    "- Small batch offers regularizing effect(adds noise to learning process), but high run time\n",
    "- Multicore architectures require a minimum batch size to be effective\n",
    "- Bigger batch size = faster computation\n",
    "- Batch size is usually a power of two\n",
    "- Dependent on your machine and model size\n",
    "\n",
    "Good starting point: {32, 64} for CPU or {128, 256} for GPU\n",
    "    \n",
    "> … [batch size] is typically chosen between 1 and a few hundreds, e.g. [batch size] = 32 is a good default value\n",
    "_Practical recommendations for gradient-based training of deep architectures, 2012_ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "In general:\n",
    "<center><img src=\"../images/loss_functions_table.png\" width=\"700\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates (optimizer)\n",
    "\n",
    "**SGD** \n",
    "- Strength: often best generalization \n",
    "- Weakness: \n",
    "    - long training time\n",
    "    - sensitive to initialization & learning rate parameter\n",
    "    \n",
    "**SGD with momentum**\n",
    "- Strength: overcomes sensitivity to initialization\n",
    "- Weakness:\n",
    "    - sensitive to learning rate parameter $\\alpha$ \n",
    "    - sensitive momentum parameter $\\beta$ \n",
    "        \n",
    "**Adam**\n",
    "- Strength:\n",
    "    - good default settings\n",
    "    - works well on sparse features\n",
    "    - automatically decays learning rate parameter\n",
    "- Weakness: generalizes worse\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates (optimizer)\n",
    "\n",
    "#### Recommendation\n",
    "* Adam is a good default choice\n",
    "* SGD with momentum if you have the resources to find a good learning rate\n",
    "* Sparse data: adaptive gradient methods such as Adam, RMSprop or AdaGrad\n",
    "* Look at state-of-the-art papers for your dataset and/or task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates (optimizer)\n",
    "**Example:** Say you want to train a Generative Adversarial Network (GAN) to perform super-resolution on a set of images. After some research you stumble upon this paper in which the researchers used the Adam optimizer to solve the exact same problem. Wilson et al. argue that training GANs does not correspond to solving optimization problems and that Adam may be well-suited for such scenarios. Which optimizer do you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   &rarr; Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates (optimizer)\n",
    "**Example:** For a project at your current job you have to classify written user responses into positive and negative feedback. You consider to use bag-of-words as input features for your machine learning model. Since these features can be very sparse you decide to go for an adaptive gradient method, which leaves Adam, RMSprop or AdaGrad. You also have a limited time frame for your project. Adam and RMSprop both have more tunable parameters than AdaGrad. Which optimizer do you choose?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   &rarr; AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialization \n",
    "\n",
    "In case of ReLU activations go for He initialization, otherwise use Xavier (also called Glorot initialization)\n",
    "![](https://pouannes.github.io/initialization/converge_22layers.png#center) \n",
    "Source: Kaiming He's paper comparing Xavier and He initialization with ReLU as the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture\n",
    "\n",
    "**Dropout** \n",
    "- `keep_prob`\n",
    "    * 0.5 for fully-connected nets\n",
    "    * 0.1-0.2 for convolutional nets\n",
    "- When to use:\n",
    "    * after every fully-connected layer (except the last)\n",
    "    * convnet: not necessary or only in the lower layers\n",
    "\n",
    "_Important: disable during test time_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Architecture\n",
    "\n",
    "**Activation**\n",
    "- ReLU for hidden layers\n",
    "- Sigmoid, softmax or linear for your final layer\n",
    "\n",
    "_Exception:_ if the fraction of 'dead' neurons is high, go for Leaky ReLU or Maxout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Architecture\n",
    "\n",
    "**Convolution, activation and batch normalization**\n",
    "- Original paper: conv-bn-act \n",
    "- Practice: conv-act-bn \n",
    "\n",
    "![](../images/convbnact.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Architecture\n",
    "**Pooling**\n",
    "- Periodically insert between convolutional layers or blocks\n",
    "- Max pooling more effective than average pooling\n",
    "- Global average pooling or global max pooling can be used as an alternative to \"flatten\" the feature maps for the last layer.\n",
    "- Cheaper alternative to convolution with stride\n",
    "\n",
    "![](https://qph.fs.quoracdn.net/main-qimg-1afcb29913e4a667e78790c597e27712)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Architecture\n",
    "\n",
    "![](../images/example_conv_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "- **Dropout** and batch normalization\n",
    "- Data augmentation\n",
    "- Early stopping \n",
    "<center><img src=\"../images/dropout.png\" width=\"700\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regularization\n",
    "- Dropout and batch normalization\n",
    "- **Data augmentation**\n",
    "- Early stopping \n",
    "\n",
    "![](../images/ender-translated.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regularization\n",
    "- Dropout and batch normalization\n",
    "- **Data augmentation**\n",
    "- Early stopping \n",
    "\n",
    "A technique to increase the diversity of your training set by applying random (but realistic) transformations, such as image rotation.\n",
    "\n",
    "![](../images/ender-rotated.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regularization\n",
    "- Dropout and batch normalization\n",
    "- **Data augmentation**\n",
    "- Early stopping \n",
    "![center](../images/dataaugmentation.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Regularization\n",
    "- Dropout and batch normalization\n",
    "- Data augmentation\n",
    "- **Early stopping**\n",
    "\n",
    "![half center](../images/overfitting.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer learning\n",
    "A model developed for a task is reused as the starting point for another task.\n",
    "![center third](../images/andrew_ng_nips_2016_transfer_learning-1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transfer learning\n",
    "\n",
    "![center half](../images/pretrained_ops_accuracy.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Transfer learning\n",
    "\n",
    "**Transfer learning as a feature extractor**\n",
    "\n",
    "    - Remove last fully-connected layer\n",
    "    - Treat the rest as a feature extractor: get output of the last layer with your dataset \n",
    "    - Train a linear classifier (e.g. linear SVM or Softmax) these outputs\n",
    "    \n",
    "**Transfer learning with fine-tuning**\n",
    "\n",
    "    - Remove last layer\n",
    "    - Retrain weights of earlier layers (but not all)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Question\n",
    "### What type of transfer learning would you use in these scenarios:\n",
    "\n",
    "a) New dataset is _small_ and _similar_ to original dataset\n",
    "\n",
    "b) Net dataset is _large_ and _similar_ to original dataset\n",
    "\n",
    "c) New dataset is _small_ but _very different_ from original dataset\n",
    "\n",
    "d) New dataset is _large_ and _very different_ from original dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a) Feature extractor\n",
    "\n",
    "b) Fine tune\n",
    "\n",
    "c) Feature extractor from earlier in the network\n",
    "\n",
    "d) Fine tune through entire network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# General heuristics\n",
    "- Use pre-trained networks whenever possible\n",
    "- More data is better, but quality matters\n",
    "- Rarely helps to go deeper than 3 or 4 layers\n",
    "- Larger networks contain significantly more local minimum\n",
    "- Overfitting: regularization > smaller neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Question\n",
    "### If your Neural Network model seems to have high bias (underfitting), what of the following would be promising things to try? (More than one can apply)\n",
    "\n",
    "    a) Get more test data\n",
    "    b) Add regularization\n",
    "    c) Get more training data\n",
    "    d) Make the Neural Network deeper \n",
    "    e) Increase the number of units in each hidden layer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   &rarr; d & e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Question\n",
    "### Increasing the parameter keep_prob in Dropout from  0.5 to 0.6 will likely cause the following (choose two):\n",
    "\n",
    "    a) Increase the regularization effect\n",
    "    b) Decrease the regularization effect\n",
    "    c) Increase training set error\n",
    "    d) Decrease the training set error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   &rarr; b & d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Monitoring\n",
    "\n",
    "**Score**\n",
    "\n",
    "- *Train/val score*: track the validation/training score to determine the amount of over fitting.\n",
    "\n",
    "- Ratio of updated weights\n",
    "- Activation/gradient distributions per layer\n",
    "\n",
    "![center](../images/overfitting.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Monitoring -  Tensorboard\n",
    "\n",
    "- Learning curves\n",
    "- Computational graphs\n",
    "- Weights\n",
    "<center><img src=\"../images/tensorboard.png\" width=\"400\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Monitoring - Tensorboard\n",
    "\n",
    "- Learning curves\n",
    "- Computational graphs\n",
    "- Embeddings\n",
    "- Weights\n",
    "\n",
    "In Keras:\n",
    "\n",
    "```python\n",
    "keras.callbacks.TensorBoard(log_dir='./logs')\n",
    "```\n",
    "Befor starting you training session, start Tensorboard with the following bash command:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir nameOfDirectory\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Monitoring - Visdom\n",
    "\n",
    "![half center](../images/visdom.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "- Use neural networks for unstructured data\n",
    "- Transfer learning helps you kick-start your problem\n",
    "- Many things to consider, but a good library (e.g. Keras) will help you\n",
    "\n",
    "![](../images/keras.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
