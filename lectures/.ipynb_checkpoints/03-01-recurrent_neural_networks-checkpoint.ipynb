{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent neural network (RNN)\n",
    "\n",
    "- contains at least one feed-back connection\n",
    "- enables the neural network to do temporal processing and learn sequences\n",
    "\n",
    "![center quarter](../images/rnn_loop.png)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Order in sequences\n",
    "\n",
    "### The order of a sequence holds information\n",
    "\n",
    "> proudly part of Xebia Group\n",
    "\n",
    "vs\n",
    "\n",
    "> Group part of Xebia proudly\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Feed-forward üíî sequences\n",
    "\n",
    "We need a different kind of unit!\n",
    "\n",
    "![center quarter center](../images/feedforward-sequence.png)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent ‚ù§Ô∏è sequences\n",
    "\n",
    "Internal loop feeds back the previous state \n",
    "\n",
    "![three_quarters center](../images/rnn-architecture.png)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Formal comparison\n",
    "\n",
    "**Feed forward**\n",
    "\n",
    "<img src=\"../images/formula-feedforward.png\" style=\"height: 50px; display: block; margin-left: auto !important; margin-right: auto !important;\" align=\"center\"/>\n",
    "\n",
    "**Recurrent**\n",
    "\n",
    "<img src=\"../images/formula-recurrent.png\" style=\"height: 56px; display: block; margin-left: auto !important; margin-right: auto !important;\" align=\"center\"/>\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "Network architectures:\n",
    "\n",
    "- Simple RNNs\n",
    "- Long short-term memory\n",
    "- Gated recurrent units\n",
    "\n",
    "Use cases:\n",
    "\n",
    "- Forecasting\n",
    "- Classification\n",
    "- Outlier detection\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "<img src=\"../images/rnn_unit.png\" align='right' width='300'>\n",
    "\n",
    "- Proposed in the 80s for modeling time series\n",
    "\n",
    "- An RNN does not start its \"thinking\" from scratch\n",
    "\n",
    "- Networks can persist information\n",
    "\n",
    "- Take earlier inputs into account when making predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Language translation\n",
    "\n",
    "![three_quarters center](../images/google-translate.png)\n",
    "  \n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Image captioning\n",
    "\n",
    "![half center](../images/image-captioning.png)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Speech recognition\n",
    "\n",
    "![half center](../images/speech_recognition.jpg)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Session-based recommendations\n",
    "\n",
    "![center](../images/gru4rec.png)\n",
    "\n",
    "<sup>Source: [Hidasi et. al, 2015 \"\n",
    "Session-based Recommendations with Recurrent Neural Networks\"](https://arxiv.org/abs/1511.06939)<sup/>\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Sentiment analysis\n",
    "\n",
    "\n",
    "![center three_quarters](../images/sentiment-neuron.gif)\n",
    "\n",
    "<sup>Source: [Unsupervised Sentiment Neuron](https://blog.openai.com/unsupervised-sentiment-neuron/)<sup/>\n",
    "    \n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Entity recognition\n",
    "\n",
    "\n",
    "![center three_quarters](../images/entity_recognition.gif)\n",
    "\n",
    "<sup>Source: [DL experiment for a GDD client by Marcel Raas](https://godatadriven.com/players/marcel-raas)<sup/>\n",
    "    \n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Modeling sequences\n",
    "\n",
    "- Feedforward: activation determined by the input\n",
    "- RNN: architecture contains loops\n",
    "- RNN: activation might be determined also by its own activation at an earlier time. \n",
    "\n",
    "> A RNN can be thought of as multiple copies of the same network linked through time\n",
    "\n",
    "![center three_quarters](../images/RNN_unrolled.png)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Modeling sequences\n",
    "\n",
    "- A RNN provides a natural and flexible architecture for modeling all kinds of sequence data\n",
    "\n",
    "![center half](../images/rnn_sequence.jpeg)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# One to many example task?\n",
    "\n",
    "![center](../images/rnn_one_to_many.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Image captioning\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Many to one example task?\n",
    "\n",
    "![center](../images/rnn_many_to_one.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Sentiment analysis\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Many to many example task?\n",
    "\n",
    "![center](../images/rnn_many_to_many_a.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Language translation\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Many to many example task?\n",
    "\n",
    "![center](../images/rnn_many_to_many_b.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> Video frames classification\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Simple RNN (many-to-one)\n",
    "\n",
    "![third center](../images/simple_recurrent.png)\n",
    "\n",
    "$$\\begin{align}\n",
    "a_t &= \\varphi(W_h\\cdot x_t + W_r\\cdot a_{t-1})\\\\\n",
    "y_t &= W_o\\cdot a_t\n",
    "\\end{align}$$\n",
    "\n",
    "- One-step ahead forecasting: $x_t\\rightarrow y_{t-1}$\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Backpropagation through time ([BPTT](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf))\n",
    "\n",
    "- Extension of the backpropagation algorithm for RNNs\n",
    "- Error is propagated through time\n",
    "\n",
    "$$\\frac{\\partial L}{\\partial w} = \\Sigma_t\\frac{\\partial L_t}{\\partial w}$$\n",
    "\n",
    "Backpropagation of the $\\delta$ error vectors through the network.\n",
    "\n",
    "![center third](../images/bptt_recurrent.png)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter sharing\n",
    "\n",
    "Note that we do not have serperate weights at each time step\n",
    "- It would require a lot of resources, if there parameters are not shared. \n",
    "- We can generalize to sequences of different lengths.\n",
    "- Reflects the fact that we are performing the same task at each step, as a result, we don't have to relearn the rules at each point in the sequnce\n",
    "- Oftentimes, components of a sequences operate the same across the sequence. For instance, in NLP:\n",
    "\n",
    "                                                     \"On Monday it was snowing\"\n",
    "\n",
    "                                                     \"It was snowing on Monday\"\n",
    "    *i.e. we observe order, not position.* \n",
    "\n",
    "![center third](../images/bptt_recurrent.png)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN Exercise\n",
    "[Exercise: RNN forecast airline passengers](../exercises/03-01-rnn_forecast_airline_passenger.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
