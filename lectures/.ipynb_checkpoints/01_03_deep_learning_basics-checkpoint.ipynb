{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning  Basics\n",
    "\n",
    "![footer_logo](../images/logo.png)\n",
    "\n",
    "## Goal\n",
    "\n",
    "- Become familiar with common components of a neural network.\n",
    "- Understand the effects that different hyperparamter choices can have on the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning  Basics\n",
    "\n",
    "![footer_logo](../images/logo.png)\n",
    "\n",
    "## Program\n",
    "\n",
    "**Architecture**: \n",
    "- Fully-connected layers\n",
    "- Activation function\n",
    "- Bias\n",
    "- Batch normalization layer\n",
    "- Dropout layer\n",
    "\n",
    "**Hyperparameters**: \n",
    "- loss function\n",
    "- gradient descent type\n",
    "- weight updates\n",
    "- weight initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture \n",
    "\n",
    "We will discuss the following neural network components\n",
    "- Fully-connected layers\n",
    "- Activation function\n",
    "- Bias\n",
    "- Batch normalization layer\n",
    "- Dropout layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully-Connected layer \n",
    "\n",
    "All inputs of one layer connected to every activation unit of the next layer. \n",
    "\n",
    "Also known as _Linear_ or _Dense_ layer\n",
    "\n",
    "<center><img src=\"../images/deep_learning_basics/fully_connected.jpeg\" width=\"900\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Activation\n",
    "\n",
    "Introduces non-linearity into the network. \n",
    "\n",
    "No trainable parameters.\n",
    "![](../images/deep_learning_basics/activation_sigmoid_tanh_relu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bias\n",
    "\n",
    "An additional paramter that allows you to shift the input to the activation function to the left or right (which may be critical for successful learning).\n",
    "\n",
    "![](../images/deep_learning_basics/bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Batch normalization ([Loffe et al.](http://arxiv.org/abs/1502.03167))\n",
    "\n",
    "- Normalize the layer inputs with batch normalization.\n",
    "\n",
    "- This helps to ensure all layers activated in near optimal “regime” of the activation functions.\n",
    "\n",
    "- Since the gradients’ dependency on the scale of the weights is reduced, it allows us to use higher learning rates,\n",
    "\n",
    "- which means training is accelerated, as less iterations are required to converge to a given loss value.\n",
    "\n",
    "<center><img src=\"../images/deep_learning_basics/bn_algorithm.png\" width=\"400\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Batch Norm (notes)\n",
    "\n",
    "Batch Norm learns 4 parameters\n",
    "- $\\beta$\n",
    "- $\\gamma$\n",
    "- running mean $\\mu$ (for inference stage)\n",
    "- running variance $\\sigma^2$ (for inference stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Normalization (further reading)\n",
    "\n",
    "- Weight normalization ([Salimans et al.](https://arxiv.org/abs/1602.07868))\n",
    "\n",
    "- Layer normalization ([Ba et al.](https://arxiv.org/abs/1607.06450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "The great flexibility of neural networks makes them very powerful, however this comes at the price of easily overfitting of the data.\n",
    "\n",
    "![center](../images/deep_learning_basics/overfitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Dropout\n",
    "\n",
    "- \"Drop\" neurons in the network with probability p (every mini-batch/epoch)\n",
    "\n",
    "- No trainable paramters\n",
    "![](../images/deep_learning_basics/dropout.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "- Computing the gradient is done with respect to the error, but also with respect to what all other units are doing. Therfore certain neurons may fix the mistakes of other neurons.\n",
    "- Dropout prevents over-reliance on a subset of the neurons in a layer\n",
    "- every neuron becomes more robust\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters\n",
    "\n",
    "We shall discuss the follow hyperparamter choices,\n",
    " \n",
    "- loss function\n",
    "- gradient descent type\n",
    "- weight updates\n",
    "- weight initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "#### Classification:\n",
    "- Binary cross entropy: $L_{binary} = \\frac{1}{N}\\Sigma^N_{i=1} [y_i log(\\hat{y}_i) + (1 - y_i)log(1 - \\hat{y}_i)]$\n",
    "    \n",
    "    \n",
    "- Categorical cross entropy: $L_{categorical} = \\frac{1}{N}\\Sigma^N_{i=1}\\Sigma^c_{j=i} [y_{ij}log(\\hat{y}_{ij})]$\n",
    "    \n",
    "#### Regression\n",
    "\n",
    "- Root mean squared error (RMSE):  $L_{RMSE} = \\sqrt{\\frac{1}{N}\\Sigma_{i=1}^{n}{(y - \\hat{y})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "\n",
    "- **Multi-class classification**\n",
    "    - **softmax** output layer with **categorical** cross-entropy and **one-hot** targets.\n",
    "- **Binary or multi-label classification**\n",
    "    - **sigmoid** output layer with **binary** cross-entropy and **binary** vector targets.\n",
    "- **Regression**\n",
    "    - **linear** output layer with **RMSE**\n",
    "    - Not performing? Try **discretizing output** through binning. Otherwise, go for a different learning algorithm.\n",
    "    \n",
    "<center><img src=\"../images/deep_learning_basics/loss_functions_table.png\" width=\"700\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "* Stochastic gradient descent - feed a single datapoint in at each pass.\n",
    "* Batch gradient descent - feed in the whole batch of data at each pass.\n",
    "* Mini-batch gradient descent - feed in a group of data at each pass.\n",
    "\n",
    "<center><img src=\"../images/deep_learning_basics/batch.png\" width=\"600\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent\n",
    "\n",
    "We update our model using gradient descent.\n",
    "<center><img src=\"../images/deep_learning_basics/cost_function_gradient.png\" width=\"600\"><center>\n",
    "\n",
    "\n",
    "### <center> $ w' = w - \\eta \\frac{\\partial J(w)}{\\partial w }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "\n",
    "<center><img src=\"../images/deep_learning_basics/computational_resource.png\" width=\"700\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Forward pass**: a data sample is passed forward through the network to determine a prediction\n",
    "- **Backward pass**: recursively compute the error backwards from the last layer following the chain-rule and update the weights w.r.t. the known target output.\n",
    "- **Epoch**: training the neural network with all the training data for one cycle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "**Learning rate**: small value η typically between 1.0 and 10^-6\n",
    "\n",
    "![](../images/deep_learning_basics/optimal_learning_rate.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "**Learning rate**: small value η typically between 1.0 and 10^-6\n",
    "\n",
    "![](../images/deep_learning_basics/learningrates.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates \n",
    "\n",
    "**Momentum:** take into account the gradient estimation of the previous batches\n",
    "\n",
    "_SGD with momentum, Nesterov momentum_\n",
    "    \n",
    "![](../images/deep_learning_basics/nesterov_momentum.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Momentum (further reading)\n",
    "\n",
    "The main difference is in classical momentum you first correct your velocity and then make a big step according to that velocity (and then repeat), but in Nesterov momentum you first making a step into velocity direction and then make a correction to a velocity vector based on new location (then repeat).\n",
    "\n",
    "i.e. without momentum:\n",
    "\n",
    "`vW(t+1) = - scaling * gradient_F( W(t) )`\n",
    "\n",
    "`W(t+1) = W(t) + vW(t+1)`\n",
    "\n",
    "Classical momentum:\n",
    "\n",
    "`vW(t+1) = momentum*Vw(t) - scaling * gradient_F( W(t) )`\n",
    "\n",
    "`W(t+1) = W(t) + vW(t+1)`\n",
    "\n",
    "While Nesterov momentum is this:\n",
    "\n",
    "`vW(t+1) = momentum*Vw(t) - scaling .* gradient_F( W(t) + momentum*vW(t) )`\n",
    "\n",
    "`W(t+1) = W(t) + vW(t+1)`\n",
    "\n",
    "![](../images/deep_learning_basics/momentum.png)\n",
    "\n",
    "[source](https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc#:~:text=The%20main%20difference%20is%20in,new%20location%20(then%20repeat).)\n",
    "[source](https://medium.com/konvergen/momentum-method-and-nesterov-accelerated-gradient-487ba776c987)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates \n",
    "\n",
    "**Adaptive learning rate**: adapt the learning rate based on the gradient history (removing the dependency on hyperparamter choice).\n",
    "\n",
    "_AdaGrad, AdaDelta, RMSprop_\n",
    "\n",
    "**Momentum & adaptive learning rate**: _Adam, Nadam_\n",
    "\n",
    "More on this later!\n",
    "\n",
    "![](../images/deep_learning_basics/optimizers_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## Optimizers (further reading)\n",
    "\n",
    "[UvA notes on optimizers](https://uvadlc.github.io/lectures/dec2020/lecture3.1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "![half center](../images/deep_learning_basics/optimizers.png)\n",
    "\n",
    "<!-- Different optimisers can be used with all the types of gradient descente. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "There are a few contradictory requirements:\n",
    "\n",
    "- Weights need to be small enough magnitude $\\rightarrow$ Otherwise output values explode\n",
    "- Weights need to be large enough magnitude $\\rightarrow$ Otherwise signal too weak to propagate\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"../images/deep_learning_basics/fully_connected.jpeg\" width=\"600\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "**Naive approaches**: All zero\n",
    "\n",
    "Every hidden unit will get zero signal. No matter what the input was, the output would be the same!\n",
    "\n",
    "\n",
    "<center><img src=\"../images/deep_learning_basics/fully_connected.jpeg\" width=\"600\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "**Naive approaches**: All constant (e.g. all 1.0)\n",
    "\n",
    "- Input to each neuron in a layer will be the same, \n",
    "- therefore the update each neuron in a layer receives will be the same,\n",
    "- this will prevent different neurons in a layer from learning different things.\n",
    "\n",
    "\n",
    "\n",
    "<center><img src=\"../images/deep_learning_basics/fully_connected.jpeg\" width=\"600\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialiation\n",
    "\n",
    "**Solution**: Break symmetry with a random initializaiton.\n",
    "- Xavier or Glorot init: \n",
    "$w\\sim\\sqrt{\\frac{2}{n_{in}+n_{out}}}\\cdot N(0,1)$ ([Glorot et al.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))\n",
    "- He init: $w\\sim\\sqrt{\\frac{2}{n_{in}}}\\cdot N(0,1)$ ([He et al.](http://arxiv-web3.library.cornell.edu/abs/1502.01852))\n",
    "\n",
    "_where $n_{in}$ and $n_{out}$ represent the number of in and outgoing connections_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap Hyperparameters\n",
    "* **Gradient descent**: dependent on data, usually mini-batch\n",
    "* **Error function**: dependent on the type of problem; influences final activation\n",
    "* **Weight updates**: dependent on data, usually Adam is a good optimizer choice\n",
    "* **Weight initialisation**: He or Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters Questions\n",
    "\n",
    "* Which is computationally more expensive: SGD, batch gradient descent or mini-batch gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "batch gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "* Why is cross-entropy preferred over e.g. classification error (/accuracy)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Accuracy is not a continuously differentiable function of the weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Which optimizer combines both an adaptive learning rate with momentum? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Why would you not initialize the weights to 0? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* In what cases would you use He/Xavier initialization over random initialization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook we covered,\n",
    "\n",
    "**Architecture**: \n",
    "- Fully-connected layers\n",
    "- Activation function\n",
    "- Bias\n",
    "- Batch normalization layer\n",
    "- Dropout layer\n",
    "\n",
    "**Hyperparameters**: \n",
    "- loss function\n",
    "- gradient descent type\n",
    "- weight updates\n",
    "- weight initialisation\n",
    "\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks in Keras\n",
    "Let's put our knowledge to practice.\n",
    "\n",
    "## [Keras basics](01_04_keras_basics.ipynb)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
