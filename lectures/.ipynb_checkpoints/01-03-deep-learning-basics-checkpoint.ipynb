{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Choice of hyperparameters: the way we choose to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Model architecture: structure of the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "* Stochastic gradient descent\n",
    "* Batch gradient descent\n",
    "* Mini-batch gradient descent\n",
    "\n",
    "<center><img src=\"https://nurserytovarsity.com/wp-content/uploads/2017/07/imagg6-1280x720.png\" width=\"600\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "* Stochastic gradient descent\n",
    "* Batch gradient descent\n",
    "* Mini-batch gradient descent\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*0_bWlD9aWRvZOzT7QJvHWw.png\" width=\"700\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "#### Classification:\n",
    "- Binary cross entropy: $L_{binary} = \\frac{1}{N}\\Sigma^N_{i=1} [y_i log(\\hat{y}_i) + (1 - y_i)log(1 - \\hat{y}_i)]$\n",
    "    \n",
    "    \n",
    "- Categorical cross entropy: $L_{categorical} = \\frac{1}{N}\\Sigma^N_{i=1}\\Sigma^c_{j=i} [y_{ij}log(\\hat{y}_{ij})]$\n",
    "    \n",
    "#### Regression\n",
    "\n",
    "- Root mean squared error (RMSE):  $L_{RMSE} = \\sqrt{\\frac{1}{N}\\Sigma_{i=1}^{n}{(y - \\hat{y})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "\n",
    "- **Multi-class classification**\n",
    "    - **softmax** output layer with **categorical** cross-entropy and **one-hot** targets.\n",
    "- **Binary or multi-label classification**\n",
    "    - **sigmoid** output layer with **binary** cross-entropy and **binary** vector targets.\n",
    "- **Regression**\n",
    "    - **linear** output layer with **RMSE**\n",
    "    - Not performing? Try **discretizing output** through binning. Otherwise, go for a different learning algorithm.\n",
    "    \n",
    "<center><img src=\"../images/loss_functions_table.png\" width=\"700\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "**Learning rate**: small value η typically between 1.0 and 10^-6\n",
    "\n",
    "![](../images/optimal_learning_rate.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "**Learning rate**: small value η typically between 1.0 and 10^-6\n",
    "\n",
    "![](../images/learningrates.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates \n",
    "\n",
    "**Momentum:** take into account the gradient estimation of the previous batches\n",
    "\n",
    "_SGD with momentum, Nesterov momentum_\n",
    "    \n",
    "![](../images/nesterov_momentum.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Momentum (further reading)\n",
    "\n",
    "The main difference is in classical momentum you first correct your velocity and then make a big step according to that velocity (and then repeat), but in Nesterov momentum you first making a step into velocity direction and then make a correction to a velocity vector based on new location (then repeat).\n",
    "\n",
    "i.e. without momentum:\n",
    "\n",
    "`vW(t+1) = - scaling .* gradient_F( W(t) )`\n",
    "\n",
    "`W(t+1) = W(t) + vW(t+1)`\n",
    "\n",
    "Classical momentum:\n",
    "\n",
    "`vW(t+1) = momentum.*Vw(t) - scaling .* gradient_F( W(t) )`\n",
    "\n",
    "`W(t+1) = W(t) + vW(t+1)`\n",
    "\n",
    "While Nesterov momentum is this:\n",
    "\n",
    "`vW(t+1) = momentum.*Vw(t) - scaling .* gradient_F( W(t) + momentum.*vW(t) )`\n",
    "\n",
    "`W(t+1) = W(t) + vW(t+1)`\n",
    "\n",
    "![](https://miro.medium.com/max/600/1*4F6O5Jo936tykq0M1E_Z2Q.png)\n",
    "\n",
    "[source](https://stats.stackexchange.com/questions/179915/whats-the-difference-between-momentum-based-gradient-descent-and-nesterovs-acc#:~:text=The%20main%20difference%20is%20in,new%20location%20(then%20repeat).)\n",
    "[source](https://medium.com/konvergen/momentum-method-and-nesterov-accelerated-gradient-487ba776c987)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates \n",
    "\n",
    "**Adaptive learning rate**: adapt the learning rate based on the gradient history (removing the dependency on hyperparamter choice).\n",
    "\n",
    "_AdaGrad, AdaDelta, RMSprop_\n",
    "\n",
    "**Momentum & adaptive learning rate**: _Adam, Nadam_\n",
    "\n",
    "![](../images/optimizers_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Optimizers (further reading)\n",
    "\n",
    "[UvA notes on optimizers](https://uvadlc.github.io/lectures/dec2020/lecture3.1.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "![half center](../images/optimizers.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "There are a few contradictory requirements:\n",
    "\n",
    "- Weights need to be small enough magnitude $\\rightarrow$ Otherwise output values explode\n",
    "- Weights need to be large enough magnitude $\\rightarrow$ Otherwise signal too weak to propagate\n",
    "\n",
    "![center](../images/backpropagation_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "**Naive approaches**: All zero\n",
    "\n",
    "Every hidden unit will get zero signal. No matter what the input was, the output would be the same!\n",
    "\n",
    "![center](../images/forward_pass_0.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "**Naive approaches**: All constant (e.g. all 1.0)\n",
    "\n",
    "- Input to each neuron in a layer will be the same, \n",
    "- therefore the update each neuron in a layer receives will be the same,\n",
    "- this will prevent different neurons in a layer from learning different things.\n",
    "\n",
    "![center](../images/forward_pass_0.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialiation\n",
    "\n",
    "**Solution**: Break symmetry with a random initializaiton.\n",
    "- Xavier init: \n",
    "$w\\sim\\sqrt{\\frac{2}{n_{in}+n_{out}}}\\cdot N(0,1)$ ([Glorot et al.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))\n",
    "- He init: $w\\sim\\sqrt{\\frac{2}{n_{in}}}\\cdot N(0,1)$ ([He et al.](http://arxiv-web3.library.cornell.edu/abs/1502.01852))\n",
    "\n",
    "_where $n_{in}$ and $n_{out}$ represent the number of in and outgoing connections_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap Hyperparameters\n",
    "* **Gradient descent**: dependent on data, usually mini-batch\n",
    "* **Error function**: dependent on the type of problem; influences final activation\n",
    "* **Weight updates**: dependent on data, usually Adam is a good choice\n",
    "* **Weight initialisation**: He or Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters Questions\n",
    "\n",
    "* Which is computationally more expensive: SGD, batch gradient descent or mini-batch gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "* Why is cross-entropy preferred over e.g. classification error (/accuracy)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Accuracy is not a continuously differentiable function of the weights!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Which optimizer combines both an adaptive learning rate with momentum? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Why would you not initialize the weights to 0? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* In what cases would you use He/Xavier initialization over random initialization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture \n",
    "* Fully-connected\n",
    "* Activation \n",
    "* Bias\n",
    "* Normalization\n",
    "* Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully-Connected layer \n",
    "\n",
    "All inputs of one layer connected to every activation unit of the next layer. \n",
    "\n",
    "Also known as _Linear_ or _Dense_ layer\n",
    "\n",
    "<center><img src=\"https://cs231n.github.io/assets/nn1/neural_net2.jpeg\" width=\"900\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Activation\n",
    "\n",
    "Introduces non-linearity into the network. \n",
    "\n",
    "No trainable parameters.\n",
    "![](../images/activation_sigmoid_tanh_relu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Bias\n",
    "\n",
    "An additional paramter that allows you to shift the activation function to the left or right (which may be critical for successful learning).\n",
    "\n",
    "![](https://matthewmazur.files.wordpress.com/2018/03/neural_network-7.png?w=584)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Batch Normalization\n",
    "\n",
    "Batch normalization ([Loffe et al.](http://arxiv.org/abs/1502.03167))\n",
    "\n",
    "- Normalize the layer inputs with batch normalization.\n",
    "\n",
    "- This helps to ensure all layers activated in near optimal “regime” of the activation functions.\n",
    "\n",
    "- Since the gradients’ dependency on the scale of the weights is reduced, it allows us to use higher learning rates,\n",
    "\n",
    "- which means training is accelerated, as less iterations are required to converge to a given loss value.\n",
    "\n",
    "<center><img src=\"../images/bn_algorithm.png\" width=\"700\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Batch Norm (notes)\n",
    "\n",
    "Batch Norm learns 4 parameters\n",
    "- $\\beta$\n",
    "- $\\epsilon$\n",
    "- running mean $\\mu$ (for inference stage)\n",
    "- running variance $\\sigma^2$ (for inference stage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Normalization (further reading)\n",
    "\n",
    "- Weight normalization ([Salimans et al.](https://arxiv.org/abs/1602.07868))\n",
    "\n",
    "- Layer normalization ([Ba et al.](https://arxiv.org/abs/1607.06450))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "\n",
    "The great flexibility of neural networks makes them very powerful, however this comes at the price of easily overfitting of the data.\n",
    "\n",
    "![center](../images/overfitting.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout\n",
    "\n",
    "- \"Drop\" neurons in the network with probability p (every mini-batch/epoch)\n",
    "\n",
    "- No trainable paramters\n",
    "![](../images/dropout.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout\n",
    "\n",
    "- Computing the gradient is done with respect to the error, but also with respect to what all other units are doing. Therfore certain neurons may fix the mistakes of other neurons.\n",
    "- Dropout prevents over-reliance on a subset of the neurons in a layer\n",
    "- every neuron becomes more robust\n",
    "![](../images/dropout.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "**Hyperparameters**: \n",
    "- gradient descent type \n",
    "- loss function\n",
    "- weight updates\n",
    "- weight initialisation\n",
    "\n",
    "**Architecture**: \n",
    "- Fully-connected layers\n",
    "- Activation function\n",
    "- Bias\n",
    "- (batch) normalization\n",
    "- Rgularization with dropout\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks in Keras\n",
    "Let's put our knowledge to practice.\n",
    "\n",
    "## [Keras basics](01-04-keras_basics.ipynb)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
