{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep Learning \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Choice of hyperparameters: the way we choose to train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Model architecture: structure of the layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "* Stochastic gradient descent\n",
    "* Batch gradient descent\n",
    "* Mini-batch gradient descent\n",
    "\n",
    "<center><img src=\"https://nurserytovarsity.com/wp-content/uploads/2017/07/imagg6-1280x720.png\" width=\"600\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Gradient descent\n",
    "* Stochastic gradient descent\n",
    "* Batch gradient descent\n",
    "* Mini-batch gradient descent\n",
    "\n",
    "<center><img src=\"https://cdn-images-1.medium.com/max/800/1*0_bWlD9aWRvZOzT7QJvHWw.png\" width=\"700\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "#### Classification:\n",
    "- Binary cross entropy: $L_{binary} = \\frac{1}{N}\\Sigma^N_{i=1} [y_i log(\\hat{y}_i) + (1 - y_i)log(1 - \\hat{y}_i)]$\n",
    "    \n",
    "    \n",
    "- Categorical cross entropy: $L_{categorical} = \\frac{1}{N}\\Sigma^N_{i=1}\\Sigma^c_{j=i} [y_{ij}log(\\hat{y}_{ij})]$\n",
    "    \n",
    "#### Regression\n",
    "\n",
    "- Root mean squared error (RMSE):  $L_{RMSE} = \\sqrt{\\frac{1}{N}\\Sigma_{i=1}^{n}{(y - \\hat{y})^2}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "\n",
    "- **Multi-class classification**\n",
    "    - **softmax** output layer with **categorical** cross-entropy and **one-hot** targets.\n",
    "- **Binary or multi-label classification**\n",
    "    - **sigmoid** output layer with **binary** cross-entropy and **binary** vector targets.\n",
    "- **Regression**\n",
    "    - **linear** output layer with **RMSE**\n",
    "    - Not performing? Try **discretizing output** through binning. Otherwise, go for a different learning algorithm.\n",
    "    \n",
    "<center><img src=\"../images/loss_functions_table.png\" width=\"700\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "**Learning rate**: small value η typically between 1.0 and 10^-6\n",
    "\n",
    "![](../images/optimal_learning_rate.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "**Learning rate**: small value η typically between 1.0 and 10^-6\n",
    "\n",
    "![](../images/learningrates.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates \n",
    "\n",
    "**Momentum:** take into account the gradient estimation of the previous batches\n",
    "\n",
    "_SGD with momentum, Nesterov momentum_\n",
    "    \n",
    "![](../images/nesterov_momentum.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates \n",
    "\n",
    "**Adaptive learning rate**: adapt the learning rate based on the gradient history\n",
    "\n",
    "_AdaGrad, AdaDelta, RMSprop_\n",
    "\n",
    "**Momentum & adaptive learning rate**: _Adam, Nadam_\n",
    "\n",
    "![](../images/optimizers_1.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight updates\n",
    "\n",
    "![half center](../images/optimizers.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "- Determines where your search starts\n",
    "- Too small: gradients in first layer will become small\n",
    "- Too big: activations will be extreme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Weight initialization\n",
    "\n",
    "Naive approaches: \n",
    "* All zero\n",
    "* Random uniform distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solution: **Fan-scaled random** initialization to limit output variance\n",
    "- Xavier init: \n",
    "$w\\sim\\sqrt{\\frac{2}{n_{in}+n_{out}}}\\cdot N(0,1)$ ([Glorot et al.](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf))\n",
    "- He init: $w\\sim\\sqrt{\\frac{2}{n_{in}}}\\cdot N(0,1)$ ([He et al.](http://arxiv-web3.library.cornell.edu/abs/1502.01852))\n",
    "\n",
    "_where $n_{in}$ and $n_{out}$ represent the number of in and outgoing connections_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Recap Hyperparameters\n",
    "* **Gradient descent**: dependent on data, usually mini-batch\n",
    "* **Error function**: dependent on the type of problem; influences final activation\n",
    "* **Weight updates**: dependent on data, usually Adam is a good choice\n",
    "* **Weight initialisation**: He or Xavier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters Questions\n",
    "\n",
    "* Which is computationally more expensive: SGD, batch gradient descent or mini-batch gradient descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " \n",
    "* Why is cross-entropy preferred over e.g. classification error (/accuracy)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Which optimizer combines both an adaptive learning rate with momentum? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* Why would you not initialize the weights to 0? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "* In what cases would you use He/Xavier initialization over random initialization? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Architecture \n",
    "* Fully-connected\n",
    "* Activation \n",
    "* Dropout\n",
    "* Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fully-Connected layer \n",
    "\n",
    "All inputs of one layer connected to every activation unit of the next layer. \n",
    "\n",
    "Also known as _Linear_ or _Dense_ layer\n",
    "\n",
    "<center><img src=\"https://cs231n.github.io/assets/nn1/neural_net2.jpeg\" width=\"900\"><center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Activation\n",
    "\n",
    "Introduces non-linearity into the network. \n",
    "\n",
    "No trainable parameters.\n",
    "![](../images/activation_sigmoid_tanh_relu.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dropout\n",
    "\n",
    "No trainable parameters.\n",
    "![](../images/dropout.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normalization\n",
    "\n",
    "\n",
    "- Batch normalization ([Loffe et al.](http://arxiv.org/abs/1502.03167))\n",
    "\n",
    "- Weight normalization ([Salimans et al.](https://arxiv.org/abs/1602.07868))\n",
    "\n",
    "- Layer normalization ([Ba et al.](https://arxiv.org/abs/1607.06450))\n",
    "\n",
    "<center><img src=\"../images/bn_algorithm.png\" width=\"700\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "**Hyperparameters**: \n",
    "- gradient descent type \n",
    "- loss function\n",
    "- weight updates\n",
    "- weight initialisation\n",
    "\n",
    "**Architecture**: \n",
    "- Fully-connected layers\n",
    "- Activation function\n",
    "- Dropout & (batch) normalization\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks in Keras\n",
    "Let's put our knowledge to practice.\n",
    "\n",
    "## [Keras basics](01-03-keras_basics.ipynb)\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
