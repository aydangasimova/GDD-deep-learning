{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AI vs ML vs DL: what is what?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![half center](../images/ai-ml-deeplearning.png)\n",
    "\n",
    "<sub>Source: eureka!</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# ML vs DL: what is what?\n",
    "\n",
    "![half center](https://miro.medium.com/max/1386/1*ZX05x1xYgaVoa4Vn2kKS9g.png)\n",
    "\n",
    "<sub>Source: George Seif, Medium</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Applications of Deep Learning\n",
    "* Translation \n",
    "* Image captioning\n",
    "* Text and image generation\n",
    "* Games \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications of Deep Learning\n",
    "* **Translation**\n",
    "* Object detection & image captioning\n",
    "* Text and image generation\n",
    "* Games \n",
    "\n",
    "\n",
    "![half center](../images/google-translate.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications of Deep Learning\n",
    "* Translation\n",
    "* **Object detection & image captioning**\n",
    "* Text and image generation\n",
    "* Games \n",
    "\n",
    "![half center](../images/image-captioning.png)\n",
    "![half center](https://img.codefor.nl/?url=https://tada.city/wp-content/uploads/2019/05/computer-vision-1920x802.jpg&width=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications of Deep Learning\n",
    "* Translation\n",
    "* Object detection & image captioning\n",
    "* **Text and image generation**\n",
    "* Games \n",
    "\n",
    "<img src=\"../images/notexist.png \" width=\"500\">\n",
    "\n",
    "\n",
    "Source: [thispersondoesnotexist.com](https://www.thispersondoesnotexist.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Applications of Deep Learning\n",
    "* Translation\n",
    "* Object detection & image captioning\n",
    "* Text and image generation\n",
    "* **Games** \n",
    "\n",
    "<img src=\"https://www.veto.be/cache/img/fd2c838dfada2ceebd97fa3e8d19b59c-alphago.jpg \" width=\"400\">\n",
    "<img src=\"https://cdn-../images-1.medium.com/max/1000/1*eYzPn6dO4YP9Kc-20Iw7PQ.jpeg\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# History of Deep Learning\n",
    "\n",
    "<img src=\"https://3.bp.blogspot.com/-B5owuZ0DVy8/XDXN5T6aWxI/AAAAAAAAgtU/gpuKGKJoQfsH-W6qBaK2Lla8bRIU9Jf3wCLcBGAs/s1600/Machine%2BLearning%2BHistory%2B-%2BTimeline%2B-%2BBook%2B2e.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# History\n",
    "\n",
    "- 1958 - Percentron unit - Frank Rosenblatt\n",
    "- 1986 - Backpropagation - Geoffrey Hinton\n",
    "- 1989 - LeNet Backpropagation to multi-layer perceptron - Yan LeCun\n",
    "- 1998 - LeNet-5 Convolutional neural networks - Yan Lecun\n",
    "- 2007 - Fei Fei Li Princeton ImageNet competition\n",
    "- 2009 - GPU for deep learning - Andrew Ng\n",
    "- 2011 - Demonstration of ReLu for deep neural networks - Yoshua Bengio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# History\n",
    "- 2012 - AlexNet wins ImageNet 25% to 16% error\n",
    "- 2012 - Dropout technique - Geoffrey Hinton\n",
    "- 2014 - Generative adversarial networks - Ian Goodfellow & Yoshua Bengio\n",
    "- 2015 - CNN beats human error in ImageNet 5% to 3% (ResNet)\n",
    "- 2016 - AlphaGo - Google DeepMind\n",
    "- 2017 - Capsule networks - Geoffrey Hinton\n",
    "- 2018 - \"NLP's ImageNet moment has arrived\" - Sebastian Ruder\n",
    "- 2019 - Human error for GLUE - BERT, ELMo, Open-GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# History - the rise of deep learning\n",
    "\n",
    "1. GPUs for fast computation\n",
    "\n",
    "    _(GPU for deep learning by Andrew Ng, 2009)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2. Effective neural network components\n",
    "\n",
    "    _e.g. dropout (Hinton, 2012) and ReLU (Bengio, 2011)_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "3. Large annotated datasets \n",
    "\n",
    "    _(e.g. ImageNet, 2011)_\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](../images/dog_breeds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# History - Frameworks\n",
    "![three_quarters center](../images/frameworks-time.jpg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Frameworks\n",
    "\n",
    "* Theano (2007 - 2017-ish): open source project by the University of Montréal. \n",
    "* Keras (March 2015): \n",
    "    - originally a standalone API with various backends \n",
    "    - now part of TensorFlow\n",
    "* TensorFlow (November 2015): open source library by Google\n",
    "* PyTorch (September 2016): open source library by Facebook \n",
    "\n",
    "\n",
    "<img src=\"../images/pytorch.png\" width=\"200\">\n",
    "<img src=\"../images/tensorflow.png\" width=\"200\">\n",
    "<img src=\"../images/keras.png\" width=\"200\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Neural Networks: intuition\n",
    "\n",
    "![neuron_comparison center half](../images/neuron_comparison.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks: intuition\n",
    "\n",
    "![neuron_comparison center half](../images/neuron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks\n",
    "* **Neuron (node)**: element of a network where inputs (vector **x**) are combined with weights (vector **w**), a bias (value *b*) and a non-linear activation function (σ) to produce an output value, i.e. ``` output = σ(wTx + b) ```\n",
    "* **Layer**: a column of neurons stacked together that can receive the same inputs.\n",
    "* **Hidden layers**: intermediate layers between inputs and outputs.\n",
    "* **Deep neural network**: a neural network that contains many hidden layers, and can therefore provide solutions to more complicated and subtle decision problems. \n",
    "\n",
    "\n",
    "![neuron_comparison center half](../images/neuron.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks: peceptron\n",
    "\n",
    "* Logical operator AND\n",
    "* Logical operator OR\n",
    "* Logical operator XOR \n",
    "\n",
    "| p | q | p AND q | p OR q | p XOR q | \n",
    "|---|---|---|---|---|\n",
    "| 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 1 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks: training\n",
    "\n",
    "\n",
    "*Training*: the process of tuning the weights **w** in a network by providing the network with example data; a combination of input data **X** and the target label the network should predict *y*.  \n",
    "\n",
    "* Gradient descent: process of following the gradients of the error function towards a minimum value\n",
    "* Backpropagation: fast algorithm for computing such gradients based on the chain-rule\n",
    "\n",
    "\n",
    "![center](../images/cost_function_gradient.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Neural Networks: training\n",
    "\n",
    "Epoch: \n",
    "\n",
    "1. *Forward pass*: a data sample is passed forward through the network to determine a prediction\n",
    "2. *Backward pass*: recursively compute the error backwards from the last layer following the chain-rule and update the weights w.r.t. the known target output. \n",
    "\n",
    "Requirement: all elements of the neural network should be differentiable\n",
    "\n",
    "\n",
    "![center](../images/model_diagram.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "### > Feed data through network\n",
    "\n",
    "![center](../images/forward_pass_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "### > Feed data through network\n",
    "\n",
    "\n",
    "![center](../images/forward_pass_1.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Forward pass\n",
    "### > Error = truth - output\n",
    "(or error function)\n",
    "\n",
    "![center](../images/forward_pass_2.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Local error contribution\n",
    "![center](../images/backpropagation_0.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Local error contribution\n",
    "\n",
    "\n",
    "![center](../images/backpropagation_1.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Optimize weight with local error\n",
    "\n",
    "\n",
    "![center](../images/backpropagation_2.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Backpropagation\n",
    "### > Optimize weight with local error\n",
    "\n",
    "\n",
    "![center](../images/backpropagation_3.gif)\n",
    "\n",
    "<sub>*Ryszard Tadeusiewcz \"Sieci neuronowe\", Kraków 1992*</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "* Difference AI vs. ML vs Deep Learning\n",
    "* History of DL \n",
    "* Intuition to neural networks: perceptron\n",
    "* Backpropagation\n",
    "\n",
    "### [Exercise: gradient descent for XOR perceptron](exercises/01_exercises_xor_perceptron.ipynb)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
