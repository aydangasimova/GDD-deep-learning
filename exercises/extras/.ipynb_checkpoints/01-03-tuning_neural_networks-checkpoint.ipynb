{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../../css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Neural Networks\n",
    "\n",
    "\n",
    "![footer_logo](../../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "- Solve a practical problem from scratch\n",
    "- Familiarize yourself with advanced settings and parameter tunings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Dataset\n",
    "\n",
    "We will use a dataset containing customer information and whether they stopped being a customer with the bank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../../data/Churn_Modelling.csv')\n",
    "\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we can start\n",
    "\n",
    "- Separate relevant features and the target variable\n",
    "- Convert categorical variables\n",
    "- Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variable encoding we will use some preprocessing tools from *sklearn library*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#encoding genders\n",
    "labelencoder = LabelEncoder()\n",
    "X[:, 2] = labelencoder.fit_transform(X[:, 2])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to replace 3 country categories with 2 dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Geography'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating dummies for each country\n",
    "from sklearn.compose import ColumnTransformer \n",
    "\n",
    "ct = ColumnTransformer([(\"onehot\", OneHotEncoder(drop='first'),[1])], remainder=\"passthrough\") # The arg ([1]) is the list of columns we want to transform in this step\n",
    "X = ct.fit_transform(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "\n",
    "Before we can scale the features, we need to perform the train-test split.\n",
    "\n",
    "**(!)** We need to make sure that no information from the test set leaks into the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test) #note here how the scaler is fit only on the train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a basic Neural Network\n",
    "\n",
    "Remember that it is always good to start small and see what performance we can achieve with a relatively simple model. A Neural Network with two hidden layers is a good starting point.\n",
    "\n",
    "Choosing the right number of nodes in the hidden layers is always a matter of trial and error. However, there is a rule of thumb that often gives good results: *taking the number of nodes as the average between the number of inputs and the outputs of our NN*. In this case we have 11 inputs and 1 output, so (11+1)/2 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_1(m_name):\n",
    "    \n",
    "    model = Sequential(name=m_name)\n",
    "    model.add(layers.Dense(name=\"FullyConnected_1\", units=6, activation=\"relu\", input_dim=11))\n",
    "    model.add(layers.Dense(name=\"FullyConnected_2\", units=6, activation=\"relu\"))\n",
    "    model.add(layers.Dense(name=\"FullyConnected_OutputLayer\", units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = make_model_1(\"SimpleModel\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(11)\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=10,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    ")\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is actually this test accuracy? Let's have a look at the target variable's class proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Exited'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "79.6% of people did not exit. This means that if we would simply predict everyone to not exit, we would be right in 79.6% cases! This is an important benchmark.\n",
    "\n",
    "Let's also have a look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5) #returns a Boolean for each value!\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm/cm.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, fmt='g', ax=ax);  #annot=True to annotate cells, ftm='g' to disable scientific notation\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels'); ax.set_ylabel('True labels') \n",
    "ax.set_title('Confusion Matrix') \n",
    "ax.xaxis.set_ticklabels(['continued', 'exited']); ax.yaxis.set_ticklabels(['continued', 'exited']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the model\n",
    "\n",
    "The first model relied on a relatively simple structure and default parameters. \n",
    "\n",
    "    Which model parameters can we set differently to try improving performance?\n",
    "    \n",
    "Let's consider a number of such parameters, how to change them and what consequences it may have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers and nodes\n",
    "\n",
    "By increasing the number of hidden layers and nodes in each layer, we can allow our model to capture more complex relationships. At the same time we are risking getting more overfitting and more computational costs, so we should still be cautious with these.\n",
    "\n",
    "Extra hidden layers can be simply added using `model.add(layers.Dense(...`, and the number of nodes is the correspondent `units=` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropouts\n",
    "\n",
    "Dropouts have became one of the most popular regularization options. It is usually a good idea to use them for each layer other than the output layer. The dropout percentage may be however hard to choose. Usually values between 0.1 and 0.5 could be used.\n",
    "\n",
    "We can simply add Dropouts by adding `model.add(layers.Dropout())` after each relevant layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different activation functions\n",
    "\n",
    "`leaky relu` activation function can take care of the lack of learning when the current weights lock us out in relu's flat part. We can simply [change this](https://keras.io/activations/) by tuning relu's `alpha` parameter. \n",
    "\n",
    "`tanh` activation function can allow us to capture more symmetrical effects and output zero on average, which sometimes helps convergence.\n",
    "\n",
    "Both of these options are however more computationally expensive than `relu`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight initialization\n",
    "\n",
    "We can change the way the weights are initialized to speed up convergence and avoid local minima. This setting is changed using `kernel_initializer` option for each layer. Note that By default Keras already uses `glorot_uniform` kernel_initializer. We can alternatively use `he_uniform`, which sometimes works better with `relu` activation functions. See the full list of possible [kernel initializers here](https://keras.io/initializers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "Stop training when performance on validation data starts increasing. Simple, yet very effective\n",
    "\n",
    "In Keras this can be achieved by adding `EarlyStopping(monitor='val_loss')` into the `callbacks=[]` option of the `model.fit()` step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "\n",
    "`adam` has shown to be one of the best optimizers in many situations. In some cases however we may want to experiment and use `optimizer='rmsprop'` and `optimizer='nadam'`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch size and Epochs\n",
    "\n",
    "Fitting the model over more epochs can ensure that we do not stop too early (loss still decreasing). Too many epochs however often result in overfitting.\n",
    "\n",
    "Batch size is usually chosen between 10 and 256 (often as powers of 2), and can be experimented with. The size of the dataset matters for it as well, with smaller datasets usually working better with smaller batch sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a better(?) model\n",
    "\n",
    "Now let's try to combine some of these settings to create a better model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(99)\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "\n",
    "def make_model_2(m_name):\n",
    "    \n",
    "    model = Sequential(name=m_name)\n",
    "    model.add(layers.Dense(name=\"FullyConnected_1\", kernel_initializer='he_uniform', units=24, input_dim=11))\n",
    "    model.add(layers.LeakyReLU(alpha=0.25))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(name=\"FullyConnected_2\", units=12, activation=\"tanh\"))\n",
    "    model.add(layers.Dropout(0.25))\n",
    "    model.add(layers.Dense(name=\"FullyConnected_OutputLayer\", units=1, activation=\"sigmoid\"))\n",
    "    \n",
    "    return model\n",
    "\n",
    "model2 = make_model_2(\"BetterModel\")\n",
    "model2.compile(optimizer='nadam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run Tensorboard use `tensorboard --logdir=tb/logs` in the terminal (make sure you are in the right directory). After running it copy the provided link into your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(999)\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "log_dir = os.path.join('tb/logs', f\"run_{time.time()}\")\n",
    "\n",
    "model2.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_split=0.1,\n",
    "    verbose=0,\n",
    "    callbacks=[es, TensorBoard(log_dir, write_graph=True)],\n",
    ")\n",
    "score2 = model2.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(\"Test loss:\", score2[0])\n",
    "print(\"Test accuracy:\", score2[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not seem to be any better! Seems like making our model more complex has only hurt the test set performance. Perhaps a different / smarter set of parameters would do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "\n",
    "With so many parameters to choose from, how do we know which ones are the best? A good idea could be to try various parameters and select the combination that results in best performance. This can be achieved using Grid search. The number of parameters however should be chosen with caution as each extra parameter dimension may increase the number of models to train exponentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "#this allows using Keras classifiers inside sklearn!\n",
    "\n",
    "from sklearn.model_selection import  GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model_gs(optimizer, units_num, activation_type, third_layer=False):\n",
    "    \n",
    "    from tensorflow.keras.layers import Activation, Dense\n",
    "    \n",
    "    model = Sequential() \n",
    "\n",
    "    model.add(Dense(units=units_num, activation=activation_type, input_dim=11))\n",
    "\n",
    "    model.add(Dense(units=units_num, activation=activation_type))\n",
    "    \n",
    "    if third_layer:\n",
    "        model.add(Dense(units=units_num, activation=activation_type))\n",
    "    \n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "#below we define how the classifier is fitted when called\n",
    "model_gs = KerasClassifier(build_fn = make_model_gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters= {'batch_size':[25,16],\n",
    "             'epochs':[10, 30, 60],\n",
    "             'units_num':[6, 12, 24],\n",
    "             'activation_type':['relu', 'tanh'],\n",
    "             'optimizer': ['rmsprop', 'adam', 'nadam']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator = model_gs,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grid_search=grid_search.fit(X_train, y_train)\n",
    "\n",
    "best_parameters=grid_search.best_params_\n",
    "best_accuracy=grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "- Tuning parameters of a deep learning model is really an art\n",
    "- Luckily there are multiple heuristics that we can use to try to make our models better (or at least to not make them worse)\n",
    "- Searching over a set of parameters to find the best ones is usually a better idea than trying to guess them yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove tensorboard files\n",
    "shutil.rmtree('tb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
