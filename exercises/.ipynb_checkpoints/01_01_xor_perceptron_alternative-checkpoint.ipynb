{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with NumPy \n",
    "\n",
    "The goal of this notebook is to implement your own version of a multilayer perceptron (neural network) with NumPy and tune the weights with gradient descent. The target is to create a XOR perceptron.\n",
    "\n",
    "### XOR\n",
    "A XOR gate (or exclusive OR) is a logic gate that returns true (or 1) when one, and only one, of the inputs of the gate is true. Otherwise it returns false.\n",
    "\n",
    "| A | B | A XOR B | \n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "Whereas other logical operators (such as AND and OR) can be modeled with a single layer perceptron, the XOR operator is more complex and must be modeled with a multi-layer perceptron. \n",
    "\n",
    "### Step 1. Encode your data\n",
    "Encode your input data **X** and expected output data *y*. What is your input data? What is your output data? How can you encode this with a NumPy array? \n",
    "\n",
    "*Hint*: Ensure your two arrays have a similar number of dimensions. Check this with `X.ndim` and `y.ndim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_encode.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.  Create your weights\n",
    "Create a multi-layered perceptron with three layers: input layer, hidden layer and output layer, and define the training hyperparameters. \n",
    "\n",
    "- What number neurons should the input layer have? \n",
    "- What number of neurons should the output layer have? \n",
    "- What number of neurons should the hidden layer have? \n",
    "\n",
    "\n",
    "Initialize your weights. What shape should your weights? What type of initialization would you use? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_weights.py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Define your activation\n",
    "Define your activation function. In this case, we will be using a sigmoid activation:\n",
    "\n",
    "$\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "As we will be using this for backpropagation, we implement the derivative of the sigmoid function: \n",
    "\n",
    "$\\frac{d}{dx}\\sigma(x) = \\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$\n",
    "\n",
    "However, in this case, as you apply the derivative sigmoid function on the *activated* output of a layer, your input to the derivative function is _already activated_. This means applying $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$ to the layer output is equivalent to applying $\\sigma'(x) = x * (1 - x)$ to the _activated_ layer output. Our implementation of `sigmoid_derivative` can therefore be simplified to $\\sigma'(x) = x * (1 - x)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): \n",
    "    return ...\n",
    "    \n",
    "def sigmoid_derivative(x):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_sigmoids.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tune the weights\n",
    "Next, let's train our model! This consists of three steps: \n",
    "1. Forward pass\n",
    "2. Backpropagation\n",
    "3. Update the weights\n",
    "\n",
    "#### Forward pass\n",
    "For each consecutive layer in the multi-layer perceptron, the weights are multiplied with their input, and the associated bias is added. These are passed through the activation function (sigmoid) to determine the output of a layer. The output of this layer is then used as the input for the next layer.\n",
    "\n",
    "$\\text{output}_\\text{hidden} = \\sigma(x \\cdot w_\\text{hidden})$ \n",
    "\n",
    "$\\text{output}_\\text{output} = \\sigma(\\text{output}_\\text{hidden} \\cdot w_\\text{output} )$\n",
    "\n",
    "#### Backpropagation\n",
    "For backpropagation, you will want to calculate the error for a given layer and calculate the delta by multiplying the error with the output of the sigmoid derivative function applied to the output of that layer. It is called _back_ propagation because you start with the error (and delta) of the last layer, and work your way back. \n",
    "\n",
    "Output layer: \n",
    "* $\\text{error}_\\text{output} = y_\\text{true} - y_\\text{predicted}$\n",
    "\n",
    "* $\\delta_\\text{output} = \\text{error}_\\text{output} \\times \\sigma'(\\text{output}_\\text{output})$ \n",
    "\n",
    "Hidden layer:\n",
    "* $\\text{error}_\\text{hidden} = \\delta_\\text{output} \\cdot w_\\text{output}^T$\n",
    " \n",
    "* $\\delta_\\text{hidden} = \\text{error}_\\text{hidden} \\times \\sigma'(\\text{output}_\\text{hidden})$ \n",
    "\n",
    "\n",
    "#### Update the weights\n",
    "Use the deltas calculated in the previous step to update the weights.\n",
    "\n",
    "* $w_\\text{hidden} = w_\\text{hidden} + (x^T \\cdot \\delta_\\text{hidden})$\n",
    "* $w_\\text{output} = w_\\text{output} + (\\text{output}_\\text{hidden}^T \\cdot \\delta_\\text{output})$\n",
    "\n",
    "_Tip:_ the `@` operator in NumPy is a shortcut for `np.dot`. This allows you to do matrix multiplications ([see documentation](https://numpy.org/doc/stable/reference/generated/numpy.dot.html)).\n",
    "\n",
    "_Tip:_ `x.T` will give you the transpose of `x` if `x` is a `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10000\n",
    "for _ in range(epochs):\n",
    "    # Forward pass. \n",
    "    hidden_layer = ...\n",
    "    hidden_activated = ...\n",
    "\n",
    "    output_layer = ...\n",
    "    output_activated = ...\n",
    "    y_hat = output_activated\n",
    "    \n",
    "    # Backpropagation / error calculation\n",
    "    error_output = ...\n",
    "    delta_output = ...\n",
    "    \n",
    "    error_hidden = ...\n",
    "    delta_hidden = ...\n",
    "    \n",
    "    # Update weights. \n",
    "    output_weights += ...\n",
    "    hidden_weights += ...\n",
    "    \n",
    "print(y)\n",
    "y_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_network.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Evaluate\n",
    "Evaluate! Note that in our evaluation code, we've called the output of the final layer of the neural network `y_hat`. Feel free to give the variable a different name, but change it accordingly in the evaluation code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5. Evaluate\n",
    "for i, input_pair in enumerate(X):\n",
    "    target = y[i][0]\n",
    "    predicted_output = y_hat[i][0]\n",
    "    print(f'Input: {input_pair}, target: {target}, predicted output: {predicted_output} ({np.round(predicted_output).astype(np.int16)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../answers/perceptron.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
