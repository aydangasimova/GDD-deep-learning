{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Advanced\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "The goal of this Notebook is to dive deeper in the Keras API and touch some of the more advanced topics.\n",
    "We'll cover:\n",
    "\n",
    "1. Functional API\n",
    "2. Large datasets with Keras\n",
    "3. Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Functional API\n",
    "\n",
    "Only sequentially using `.add()` limits the complexity of your neural networks.\n",
    "Keras has other API's to solve that, the [functional API](https://keras.io/getting-started/functional-api-guide/) really helps with:\n",
    "\n",
    "> \"defining complex models, such as multi-output models, directed acyclic graphs, or models with shared layers.\"\n",
    "\n",
    "Let's check a minimum working example from the documentation.\n",
    "It defines a network with two hidden layers and an output layer for 10 classes.\n",
    "With the sequential API it would look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation=\"relu\", input_shape=(784,)))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the `Input` tensor, something we're initially allowed to ignore with the Sequential API, and use it when calling the first hidden layer object.\n",
    "Layers in Keras are [callable](https://en.wikipedia.org/wiki/Callable_object#In_Python) objects which mean we can call them after instantiation.\n",
    "When called layers return a tensor that contains all operations (layers and their weights) applied so far.\n",
    "\n",
    "We put the initial `inputs` and the final result `prediction` in a `Model` object that has similar functionality to a `Sequential` object.\n",
    "In this case, we don't really care about the intermediate results, so we use the dummy variable name `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "inputs = Input(shape=(784,))\n",
    "x = Dense(64, activation=\"relu\")(inputs)\n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "predictions = Dense(10, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this specific example, it's mainly more typing but this API allows you to be really flexible for non-sequential models. However, it allows you to easily define models with multiple inputs or outputs, or where you make use of the output from an earlier layer. \n",
    "\n",
    "A good example is a _Residual Network_ block. A residual network is made out of blocks where the output is copied and 'saved' for a little while, while other operations (e.g. convolutions) are applied to it. Then, these two outputs (original which has not been passed through more layers, and the version that _has_ been passed through more layers) are combined through a summation. The advantage of a ResNet architecture is that it tackles the vanishing gradient problem. \n",
    "\n",
    "![](https://developer.ridgerun.com/wiki/images/0/01/Residual_block.png)\n",
    "\n",
    "This is easily implemented in the functional API: \n",
    "```python\n",
    "inputs = Input(shape=(784,))\n",
    "x = Dense(64, activation=\"relu\")(inputs)\n",
    "x_original = x.copy() \n",
    "x = Dense(64, activation=\"relu\")(x)\n",
    "x = Add()([x, x_original])\n",
    "predictions = Dense(10, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: functional API\n",
    "> \n",
    "> Rewrite the model below with the functional API and put it into a function `make_fashion_mnist_model()`.\n",
    ">\n",
    "> We'll use this model later to classify fashion images.\n",
    "\n",
    "<img src=\"../images/fashion-mnist.png\" alt=\"flow_from\" style=\"width: 400px;\"/>\n",
    "\n",
    "> Source: [Kaggle](https://www.kaggle.com/zalando-research/fashionmnist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def make_fashion_mnist_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(28, 28, 1)))\n",
    "    model.add(MaxPool2D(pool_size=2))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(filters=32, kernel_size=2, padding=\"same\", activation=\"relu\"))\n",
    "    model.add(MaxPool2D(pool_size=2))\n",
    "\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation=\"relu\"))\n",
    "\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation=\"softmax\"))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = make_fashion_mnist_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_fashion_minst_model_functional(): \n",
    "    ...\n",
    "    \n",
    "model = make_fashion_mnist_model_function()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load ../answers/functional.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Large datasets with Keras\n",
    "\n",
    "Using numpy arrays as input can limit you once your datasets don't fit in memory anymore or if you're using  multiple devices.\n",
    "Keras has some built-in tools to help you; but training also works nicely with Python generators; and there's the integration with `tf.data.Datasets` to leverage TensorFlow's functionality.\n",
    "\n",
    "If you're not familiar with Python iterators and generators, make sure to do a bit of [reading](https://wiki.python.org/moin/Generators) before continuing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras generators\n",
    "\n",
    "The idea behind Keras generators is to not load all data in memory at once, but to generate batches of data and feed those to the model.\n",
    "For instance, instead of loading all samples, we only load 100 training points and feed those to the GPU on the fly.\n",
    "\n",
    "A good example of a Keras generator is the [`ImageDataGenerator`](https://keras.io/preprocessing/image/#imagedatagenerator-class).\n",
    "This class takes data and performs various forms of image augmentation on the fly, like whitening, shearing and zooming.\n",
    "As the name `Generator` implies, it doesn't compute these augmentations all at once, but does this in batches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "help(ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class has three methods to generated batches of augmented data:\n",
    "\n",
    "- `.flow()`: Takes data & label arrays\n",
    "- `.flow_from_dataframe()`: Takes the DataFrame and the path to a directory with the mapped images in the DataFrame\n",
    "- `.flow_from_directory()`: Takes the path to a directory\n",
    "\n",
    "We'll focus on the `.flow_from_directory()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(ImageDataGenerator.flow_from_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method expects a certain structure for it to work, read the documentation on `directory` and `classes` in the cell above.\n",
    "\n",
    "<img src=\"../images/keras_flow_from_directory.jpeg\" alt=\"flow_from\" style=\"width: 500px;\"/>\n",
    "\n",
    "> [Source](https://medium.com/@vijayabhaskar96/tutorial-image-classification-with-keras-flow-from-directory-and-generators-95f75ebe5720)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll write some fashion MNIST images to a temporary folder with this structure and use this in the following exercises.\n",
    "\n",
    "To do this we'll use some helper functions in the `load_fashion_mnist.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_fashion_mnist import save_fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dir = save_fashion_mnist(10000, 1000, 1000)\n",
    "print(f\"Dataset is written to {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -lR {temp_dir} | head -n 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Image generators\n",
    ">\n",
    "> Use the three ImageDataGenerator from the cell below to create three iterators that flow from the three `train`, `valid` and `test`. Create these iterators using `.flow_from_directory()` from the appropriate generator. \n",
    "> \n",
    "> - When creating the iterators:\n",
    ">     - Build a path for argument `directory` from `temp_dir`.\n",
    ">     - Infer the `class_mode`, `target_size` and `color_mode` from the model.\n",
    ">     - Set the `batch_size` to 32 and choose a shuffle and seed.\n",
    "> - With the already defined code, fit the Fashion MNIST model using these iterators, your loss should go lower than 1.2.\n",
    "> \n",
    "> Why are we setting `shear_range`, `zoom_range` and `horizontal_flip` on the `train_data_generator` and not on the `test_data_generator` and `valid_data_generator`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_generator = ImageDataGenerator(\n",
    "    rescale=1.0 / 255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True\n",
    ")\n",
    "test_data_generator = ImageDataGenerator(rescale=1.0 / 255)\n",
    "valid_data_generator = ImageDataGenerator(rescale=1.0 / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = train_data_generator.flow_from_directory(\n",
    "    directory=os.path.join(temp_dir, 'train'),\n",
    "    target_size=(28, 28), \n",
    "    color_mode='grayscale',\n",
    "    batch_size=8,\n",
    "    class_mode=\"categorical\",\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iterator = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iterator = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/image_data_generator.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_model = make_fashion_mnist_model()\n",
    "fashion_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "step_size_train = train_iterator.n // train_iterator.batch_size\n",
    "step_size_valid = valid_iterator.n // valid_iterator.batch_size\n",
    "\n",
    "fashion_model.fit(train_iterator,\n",
    "    steps_per_epoch=step_size_train,\n",
    "    validation_data=valid_iterator,\n",
    "    validation_steps=step_size_valid,\n",
    "    epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(fashion_model.evaluate(test_iterator, steps=30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example of a Keras generator is the [TimeseriesGenerator](https://keras.io/preprocessing/sequence/#timeseriesgenerator) that generates batches of temporal data from a sequence of data points.\n",
    "This could also been seen as generating a dataset that's possibly to big for memory: generating all possible batches from a sequence can easily be bigger that your RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We've seen how we can leverage datasets that are too big to fit in memory.\n",
    "Keras has its own generators but it's also pretty easy to build your own.\n",
    "Many file formats & interfaces also allow you to access files without loading them, like the option `mmap_mode` for [`numpy.load`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.load.html).\n",
    "If you would like to stay closer to TensorFlow, check out the guide on [Datasets](\n",
    "https://www.tensorflow.org/guide/datasets)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Callbacks\n",
    "\n",
    "Callbacks allow you to perform tasks during certain moments of training.\n",
    "For instance, you can compute performance measures like training time, or look at the states of the model to detect when it breaks down.\n",
    "\n",
    "You can pass multiple callbacks in a `list` to the `.fit()` method or your model and they'll be called at the rights times during training.\n",
    "There are six moments when a callback can be executed: at starts and/or stops of training, epochs and/or batches.\n",
    "\n",
    "\n",
    "### Built-in callbacks\n",
    "\n",
    "Let's look at two commonly used callbacks in `tensorflow.keras.callbacks`: `EarlyStopping` and `ModelCheckpoint`.\n",
    "`EarlyStopping` stops training when your model performance doesn't get better and can save you a lot of waiting time.\n",
    "`ModelCheckpoint` saves the model after every epoch to make sure your progress doesn't get lost if your training process gets killed.\n",
    "\n",
    "> #### Exercise: Built-in callbacks\n",
    ">\n",
    "> - Use the fitting procedure from the previous exercise and add the `EarlyStopping` and `ModelCheckpoint` callbacks.\n",
    "> - For `ModelCheckpoint` save only the best model and save to the variable `model_path` given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell creates an output folder where your model parameters will be saved. \n",
    "\n",
    "import shutil \n",
    "\n",
    "model_dir = os.path.join(\"..\", \"output\", \"fashion_mnist\")\n",
    "model_path = os.path.join(model_dir, \"model.h5\")\n",
    "\n",
    "if os.path.exists(model_dir):\n",
    "    shutil.rmtree(model_dir)\n",
    "os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Load & compile your model. \n",
    "... \n",
    "\n",
    "# Define your callbacks. \n",
    "...\n",
    "\n",
    "# Fit your model, with the callbacks. \n",
    "...\n",
    "\n",
    "# Evaluate your model. \n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/callbacks.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorBoard\n",
    "\n",
    "TensorBoard helps you visualize what's happening during training.\n",
    "For instance, it can visualize losses during training, weights of your layers, embedding and the computational graph.\n",
    "TensorBoard makes it easier to understand, debug, and optimize your model.\n",
    "\n",
    "<img src=\"../images/tensorboard.png\" alt=\"flow_from\" style=\"width: 600px;\"/>\n",
    "\n",
    "For Keras it's just another built-in callback.\n",
    "Using the callback writes files to a directory that can be visualized by a separate process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(tensorflow.keras.callbacks.TensorBoard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: TensorBoard\n",
    ">\n",
    "> Add the TensorBoard call back to the training of the Fashion MNIST model, set:\n",
    ">\n",
    "> - `log_dir` to the variable `run_dir` defined below\n",
    "> - Write the graph and gradients.\n",
    "> - Use the data set `(x_train, y_train), (x_test, y_test)` as defined below and train for 10 runs.\n",
    ">\n",
    "> Start training, open a terminal, make sure you're in the root folder of this project and run:\n",
    "> \n",
    "> ```\n",
    "> $ tensorboard --logdir=output/fashion_mnist\n",
    "> ```\n",
    ">\n",
    "> Start multiple runs but make sure to execute the cell with `run_dir`.\n",
    "> What happens with the losses of multiple runs?\n",
    "> With the graphs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    (x_train, y_train),\n",
    "    (x_test, y_test),\n",
    ") = tensorflow.keras.datasets.fashion_mnist.load_data()\n",
    "x_train = x_train[:10000, :, :, np.newaxis]\n",
    "x_test = x_test[:1000, :, :, np.newaxis]\n",
    "y_train = tensorflow.keras.utils.to_categorical(y_train[:10000])\n",
    "y_test = tensorflow.keras.utils.to_categorical(y_test[:1000])\n",
    "\n",
    "run_dir = os.path.join(model_dir, f\"run_{time.time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "# your code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/tensorboard.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom callbacks\n",
    "\n",
    "\n",
    "If the available callbacks don't fit your use case, it's easy to define your own.\n",
    "`LambdaCallback` can be used for simple functionality, but you can also subclass the `Callback` class.\n",
    "\n",
    "As mentioned earlier, there are six moments when a callback can be executed: at starts and/or stops of training, epochs and/or batches.\n",
    "These correspond with the arguments or methods:\n",
    "\n",
    "- `on_epoch_begin`\n",
    "- `on_epoch_end`\n",
    "- `on_batch_begin`\n",
    "- `on_batch_end`\n",
    "- `on_train_begin`\n",
    "- `on_train_end`\n",
    "\n",
    "\n",
    "If we'd want to emojify our training logs a bit, we could abuse the `LambdaCallback`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "\n",
    "def on_train_begin(_):\n",
    "    print(\"ðŸ”¥\" * 30)\n",
    "\n",
    "\n",
    "def on_train_end(_):\n",
    "    print(\"ðŸ¤–\" * 30)\n",
    "\n",
    "\n",
    "emoji_callback = LambdaCallback(\n",
    "    on_train_begin=on_train_begin, on_train_end=on_train_end\n",
    ")\n",
    "\n",
    "fashion_model = make_fashion_mnist_model()\n",
    "fashion_model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\")\n",
    "fashion_model.fit(x_train, y_train, epochs=2, callbacks=[emoji_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex logic should be captured in a subclass of the `Callback` class.\n",
    "It has six methods to overwrite and by default we get access to a few attributes: the model, trainings parameters and validation data.\n",
    "You can inspect the contents of the `tensorflow.keras.callbacks.Callback` class by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "??tensorflow.keras.callbacks.Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Custom callback\n",
    "> \n",
    "> Write a callback that prints the standard deviation of the weights in the last layer at the end of each epoch.\n",
    "> Train a new model again and observe how the loss changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load ../answers/custom_callback.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "This section showed how callbacks can be used to save & monitor your models.\n",
    "If you need a visualization tool during training, TensorBoard has easy integration with Keras.\n",
    "For custom functionality you can write your own callbacks."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
