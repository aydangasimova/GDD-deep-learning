{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent with NumPy \n",
    "\n",
    "The goal of this notebook is to implement your own version of a multilayer perceptron (neural network) with NumPy and tune the weights with gradient descent. The target is to create a XOR perceptron.\n",
    "\n",
    "### XOR\n",
    "A XOR gate (or exclusive OR) is a logic gate that returns true (or 1) when one, and only one, of the inputs of the gate is true. Otherwise it returns false.\n",
    "\n",
    "| A | B | A XOR B | \n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |\n",
    "\n",
    "Whereas other logical operators (such as AND and OR) can be modeled with a single layer perceptron, the XOR operator is more complex and must be modeled with a multi-layer perceptron. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP\n",
    "\n",
    "Below is a schematic diagram of the multi-layer perceptron we will be creating.\n",
    "\n",
    "![](../images/xor_nn.png)\n",
    "\n",
    "[source](https://github.com/omar-florez/scratch_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The learning algorithm\n",
    "\n",
    "The process we will follow to learn and evaluate our model is outlined below\n",
    "\n",
    "1. Initialize the weights randomly\n",
    "2. Iterate over the data:\n",
    "    1. Forward pass\n",
    "    2. Compute the loss\n",
    "    2. Backpropagation\n",
    "    3. Update the weights\n",
    "3. Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0. Encode your data\n",
    "\n",
    "In this notebook we have an additional step as we will create the dataset ourselves!\n",
    "\n",
    "Use NumPy to encode the A & B columns of the table above as **X** and the XOR column as *y*. \n",
    "\n",
    "*Hint*: Check the dimensions of your arrays are what you expected with `X.ndim` and `y.ndim`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_encode_m.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.  Create and initialize your weights\n",
    "We are creating a multi-layered perceptron with three layers: input layer, hidden layer and output layer. \n",
    "\n",
    "- What number of neurons should the input layer have? \n",
    "- What number of neurons should the output layer have? \n",
    "- What number of neurons should the hidden layer have? \n",
    "\n",
    "\n",
    "Initialize your weights randomly such that the size corresponds to the number of neurons in the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_weights_m.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2A. Forward pass\n",
    "\n",
    "For each consecutive layer in the multi-layer perceptron, the weights are multiplied with their input . These are passed through the activation function (sigmoid) to determine the output of a layer. The output of this layer is then used as the input for the next layer.\n",
    "\n",
    "### $z_\\text{hidden} = X W_\\text{hidden}$ \n",
    "\n",
    "### $h = \\sigma(z_\\text{hidden})$ \n",
    "\n",
    "### $z_\\text{output} = h W_\\text{output} $\n",
    "\n",
    "### $y_\\text{output} = \\sigma(z_\\text{output})$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the sigmoid activation function you will be using:\n",
    "\n",
    "### $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(input): \n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_sigmoids_m.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-linear sigmoid activation function ensures the output is between 0 and 1.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/640px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A benfit of using a sigmoid as the activation function is that it has a nice derivative, which is helpful for backpropagation.\n",
    "\n",
    "### $\\frac{d}{dx}\\sigma(x) = \\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*An example of a forward pass*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z_hidden = X @ hidden_weights\n",
    "z_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer = sigmoid(z_hidden)\n",
    "hidden_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_output =hidden_layer @ output_weights\n",
    "z_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = sigmoid(z_output)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2B. Loss function\n",
    "\n",
    "The error can be simply written as the difference between the predicted outcome ($y_\\text{output}$) and the actual outcome ($y_\\text{true} $). Mathematically, that is $ y_\\text{true} - y_\\text{output}$\n",
    "\n",
    "However, to ensure that positive and negative errors are treated the same we will take MSE.\n",
    "\n",
    "### $\\text{loss} = \\frac{1}{2}(y_\\text{true} - y_\\text{output})^2$\n",
    "\n",
    "We prefer Mean *Squared* Error to Mean *Absolute* Error and in this form (with the $\\frac{1}{2}$) as it is easier to differentiate!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2C. Backpropagation\n",
    "\n",
    "Since, there may be many weights contributing to this loss, we take the partial derivative, to find the minimum loss, with respect to each weight at a time. To do this we use backpropagation. It is called _back_ propagation because you start with the loss derivative (delta) of the last layer, and work your way back. \n",
    "\n",
    "It is possible to parallelise (compute simultaneously) the output layer weights (W31 & W32) and likewise for for the hidden layer weights (W11, W12, ..., W21, W22, ...). Therefore the derivatives we will need for gradint descent are:\n",
    "$ \\frac{\\partial \\text{loss}}{\\partial W_\\text{output}}$ & $\\frac{\\partial \\text{loss}}{\\partial W_\\text{hidden}}$\n",
    "\n",
    "\n",
    "* $W_\\text{output} = W_\\text{output} - \\eta  \\frac{\\partial \\text{loss}}{\\partial W_\\text{output}}$\n",
    "* $W_\\text{hidden} = W_\\text{hidden} - \\eta \\frac{\\partial \\text{loss}}{\\partial W_\\text{hidden}}$\n",
    "\n",
    "Using the chain rule we can break these derivatives down as follows,\n",
    "\n",
    "### $ \\frac{\\partial \\text{loss}}{\\partial W_\\text{output}} = \\ \\frac{\\partial \\text{loss}}{\\partial y_\\text{output}} \\ \n",
    "\\frac{\\partial y_\\text{output}}{\\partial z_\\text{output}} \n",
    "\\frac{\\partial z_\\text{output}}{\\partial W_\\text{output}} \n",
    "$\n",
    "\n",
    "### $ \\frac{\\partial \\text{loss}}{\\partial W_\\text{hidden}} = \\\n",
    "\\frac{\\partial \\text{loss}}{\\partial y_\\text{output}} \\ \n",
    "\\frac{\\partial y_\\text{output}}{\\partial z_\\text{output}} \n",
    "\\frac{\\partial z_\\text{output}}{\\partial h} \n",
    "\\frac{\\partial h}{\\partial z_\\text{hidden}} \n",
    "\\frac{\\partial z_\\text{hidden}}{\\partial W_\\text{hidden}}  $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute these individual derivatives,\n",
    "\n",
    "### $\\frac{\\partial \\text{loss}}{\\partial y_\\text{output}} = 2 \\times \\frac{1}{2} \\times (y_\\text{true} - y_\\text{output}) \\times(-1) = -(y_\\text{true} - y_\\text{output}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial y_\\text{output}}{\\partial z_\\text{output}} =\\\n",
    "\\sigma'(z_\\text{output} ) =\\\n",
    "\\sigma(z_\\text{output} )(1 - \\sigma(z_\\text{output} )) =\\\n",
    "y_\\text{output}(1-y_\\text{output})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial z_\\text{output}}{\\partial W_\\text{output}} = h$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial z_\\text{output}}{\\partial h} = W_\\text{output}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial h}{\\partial z_\\text{hidden}} =\\\n",
    "\\sigma'(z_\\text{hidden} ) =\\\n",
    "\\sigma(z_\\text{hidden} )(1 - \\sigma(z_\\text{hidden} )) =\\\n",
    "h(1-h)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\frac{\\partial z_\\text{hidden}}{\\partial W_\\text{hidden}} = X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\delta_1 = \\frac{\\partial \\text{loss}}{\\partial z_\\text{output}} =  \\ \\frac{\\partial \\text{loss}}{\\partial y_\\text{output}} \\ \n",
    "\\frac{\\partial y_\\text{output}}{\\partial z_\\text{output}} = -(y_\\text{true} - y_\\text{output})  y_\\text{output}(1-y_\\text{output})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta_2 = \\frac{\\partial \\text{loss}}{\\partial y_\\text{output}} \\ \n",
    "\\frac{\\partial y_\\text{output}}{\\partial z_\\text{output}} \n",
    "\\frac{\\partial z_\\text{output}}{\\partial h} \n",
    "\\frac{\\partial h}{\\partial z_\\text{hidden}} =\\\n",
    "\\delta_1 \\frac{\\partial z_\\text{output}}{\\partial h} \n",
    "\\frac{\\partial h}{\\partial z_\\text{hidden}} =\\\n",
    "\\delta_1 . W_\\text{output}^T h(1-h)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2D. Update the weights\n",
    "Using the partial derivatives calculated in the previous step, the gradient descent update formulas are:\n",
    "* $W_\\text{output} = W_\\text{output} + \\eta h ^T . \\delta_1$\n",
    "* $W_\\text{hidden} = W_\\text{hidden} + \\eta X^T . \\delta_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that $h$ and $X$ appear as transposes at the front of the matrix multiplication is due to the fact we are doing [matrix derivatives](https://towardsdatascience.com/shab-dev-story-01-trying-to-find-the-matrix-form-of-gradient-descent-via-backprop-1c2bf7fdf5fe). However, it can also be seen by doing dimensionality analysis.\n",
    "\n",
    "Implement the weight update below (you will have to set the hyperparameters too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tip:_ the `@` operator in NumPy is a shortcut for `np.matmul`. This allows you to do matrix multiplications ([see documentation](https://numpy.org/doc/stable/reference/generated/numpy.matmul.html)).\n",
    "\n",
    "_Tip:_ `x.T` will give you the transpose of `x` if `x` is a `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = _\n",
    "lr = _\n",
    "for _ in range(epochs):\n",
    "    # Forward pass. \n",
    "    z_1 = ...\n",
    "    hidden_layer = ...\n",
    "\n",
    "    z_2 = ...\n",
    "    y_output = ...\n",
    "    \n",
    "    # Backpropagation / error calculation\n",
    "    \n",
    "    delta_output = ...\n",
    "    \n",
    "    delta_hidden = ...\n",
    "    \n",
    "    # Update weights. \n",
    "    output_weights += ...\n",
    "    hidden_weights += ...\n",
    "    \n",
    "print(y)\n",
    "y_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/perceptron_network_m.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate\n",
    "Evaluate! Note that in our code, we've called the output of the final layer of the neural network `y_output`. If you used a different name you will need to adjust the code accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Evaluate\n",
    "for i, input_pair in enumerate(X):\n",
    "    target = y[i][0]\n",
    "    predicted_output = y_output[i][0]\n",
    "    print(f'Input: {input_pair}, target: {target}, predicted output: {predicted_output} ({np.round(predicted_output).astype(np.int16)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# z_1 = [[0,0]]  @ hidden_weights\n",
    "# hidden_layer = sigmoid(z_1)\n",
    "\n",
    "# z_2 = hidden_layer @ output_weights\n",
    "# sigmoid(z_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
