{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import datasets\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(707)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = 15, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build your first convolutional neural network\n",
    "\n",
    "## Goal\n",
    "The goal of this notebook is to let you build your first convolutional neural network. The example we will be working with is digit classification. Use layers such as convolutions, pooling, dropout and batch normalization for image recognition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data \n",
    "\n",
    "Like many other libraries, such as scikit-learn, `keras` includes some standard datasets to play around with. Let's load the MNIST dataset and explore what it contains: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later, when you have read this Notebook and made sense of the model, try loading the CIFAR10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X_train, y_train), (X_test, y_test) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some summary statistics on data size, type and range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Train:\\tX shape:{X_train.shape}\\tY shape:{y_train.shape}\\tType (X, y): ({X_train.dtype}, {y_train.dtype})\\tX values (max, min): ({X_train.min()}, {X_train.max()})\"\n",
    ")\n",
    "print(\n",
    "    f\"Test:\\tX shape:{X_test.shape}\\tY shape:{y_test.shape}\\tType (X, y): ({X_test.dtype}, {y_test.dtype})\\tX values (max, min): ({X_test.min()}, {X_test.max()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show some example images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "[ax.imshow(random.choice(X_train), cmap=\"gray\") for ax in plt.subplots(1, 6)[1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Dataset summary\n",
    ">\n",
    "> - How many training examples do we have?\n",
    "> - How many color channels does each picture have?\n",
    "> - What will the input size to the DNN be?\n",
    "> - What will the output size of the DNN be? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "We have to do some preprocessing on our data:\n",
    "\n",
    "- Rescale pixel values between 0 and 1\n",
    "- Input type should be float\n",
    "- There are 10 classes so in order to compute the cross entropy loss function we need to one-hot encoded vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(X_train.shape) != 4:\n",
    "    X_train = np.expand_dims(X_train, axis=3)\n",
    "if len(X_test.shape) != 4:\n",
    "    X_test = np.expand_dims(X_test, axis=3)\n",
    "\n",
    "X_train, X_test = X_train.astype(\"float\") / 255, X_test.astype(\"float\") / 255\n",
    "y_train_onehot, y_test_onehot = to_categorical(y_train), to_categorical(y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the resulting dimensions and types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Train:\\tX shape:{X_train.shape}\\tY shape:{y_train_onehot.shape}\\tType (X, y): ({X_train.dtype}, {y_train_onehot.dtype})\\tX values (min, max): ({X_train.min()}, {X_train.max()})\"\n",
    ")\n",
    "print(\n",
    "    f\"Test:\\tX shape:{X_test.shape}\\tY shape:{y_test_onehot.shape}\\tType (X, y): ({X_test.dtype}, {y_test_onehot.dtype})\\tX values (min, max): ({X_test.min()}, {X_test.max()})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_onehot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Model input & output\n",
    "- Are the input and output sizes correct from what you thought?\n",
    "- What does the fourth dimension of X represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Now that we have our data prepared, we can start building a model.\n",
    "We will use some new layers for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Model construction\n",
    "> \n",
    "> Construct a model with the instructions:\n",
    "> \n",
    "> - inputs are normalized using `BatchNormalization` followed by a `Dropout` layer with a rate of 0.3\n",
    ">   - specify the right input_shape for `BatchNormalization`\n",
    "> - then add a [2D convolutional layer](https://keras.io/layers/convolutional/) with a kernel of 3x3\n",
    ">   - also use `'same'` as `padding`, `32` `filters`, and `relu` as `activation`\n",
    "> - output from the convolutional layer goes through a [`MaxPooling` layer](https://keras.io/layers/pooling/)\n",
    "> - then `Flatten` the output and add a `Dropout` layer with a rate of 0.3\n",
    "> - connect the output to a `Dense` layer with the right amount of `units` for our classification problem\n",
    "> - followed by a `BatchNormalization` and `relu` activation function\n",
    "> - then a `DropoutLayer`\n",
    "> - and finally connect to the output layer with an `softmax` activation function\n",
    "> \n",
    "> don't forget to:\n",
    "> - use the `relu` activation function or others\n",
    "> - use `Dropout` layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NBVAL_RAISES_EXCEPTION\n",
    "def make_cnn_model():\n",
    "    model = Sequential()\n",
    "    # input layer transformation (BatchNormalization + Dropout)\n",
    "\n",
    "    # convolutional layer (Conv2D + MaxPooling2D + Flatten + Dropout)\n",
    "\n",
    "    # fully connected layer (Dense + BatchNormalization + Activation + Dropout)\n",
    "\n",
    "    # output layer (Dense + BatchNormalization + Activation)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = make_cnn_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/keras_basics_cnn.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Question:** can you explain where are the number of parameters for each layer coming from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and make sure your accuracy reaches 88%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_cnn_model()\n",
    "model.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train_onehot,\n",
    "    batch_size=5000,\n",
    "    epochs=10,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test_onehot, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "\n",
    "y_hat_onehot = model.predict(X_test)\n",
    "y_hat = np.argmax(y_hat_onehot, axis=1).astype(np.int16)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, y_hat, labels=[0,1,2,3,4,5,6,7,8,9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(multilabel_confusion_matrix(y_test, y_hat, labels=[0,1,2,3,4,5,6,7,8,9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Questions: layers\n",
    ">\n",
    "> - You've used layers like the convolutional and max-pooling layers. Can you explain what every layer does?\n",
    "> - What is the difference betwen max-pooling and average-pooling?\n",
    "> - What does batch normalization do? Why should it happen before the activation function?\n",
    "> - What is a drop out layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this exercise, we've used a convolution neural network to classify images.\n",
    "Layers like `Conv2D`, `MaxPooling2D` and `Flatten` are the building blocks for any networks working on images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
