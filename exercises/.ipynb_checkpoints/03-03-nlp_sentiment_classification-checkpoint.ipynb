{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<link rel=\"stylesheet\" type=\"text/css\" href=\"../css/custom.css\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks for Sentiment Classification\n",
    "\n",
    "\n",
    "![footer_logo](../images/logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "History matters for a lot of problems.\n",
    "If you're looking at a video with a tiny dog house, you're probably more likely to think that the weird object in the next frame is a chihuahua and not a muffin.\n",
    "\n",
    "<p style=\"text-align:center;\">\n",
    "    <img src=\"../images/chihuahua-muffin.png\" alt=\"Drawing\" style=\"width: 40%;\"/>\n",
    "</p>\n",
    "\n",
    "> Source: https://twitter.com/teenybiscuit/status/707727863571582978\n",
    "\n",
    "Feedforward networks learn their parameters once and have a fixed state, so they cannot take context in the input data into account.\n",
    "Recurrent neural networks (RNNs) also learn their parameters once, but keep a state depending on the sequence they have seemed so far.\n",
    "This makes RNNs well suited for problems with sequences, like converting speech to text: translation of a word can be helped by knowing the words that came before.\n",
    "\n",
    "In this exercise we'll apply a RNN for sentiment classification.\n",
    "We'll get some reviews from movies and try to classify if they have a positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Data\n",
    "\n",
    "Like many other libraries, `keras` includes some standard datasets to play around with.\n",
    "We'll use the IMDB dataset.\n",
    "This section shows what this dataset contains.\n",
    "\n",
    "From the [website](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) (emphasis ours): \n",
    "\n",
    "> \"Dataset of __25,000 movies reviews from IMDB, labeled by sentiment (positive/negative)__. Reviews have been preprocessed, and __each review is encoded as a sequence of word indexes__ (integers). For convenience, __words are indexed by overall frequency__ in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    ">\n",
    "> As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
    "\n",
    "\n",
    "We'll load reviews with only the 20,000 most frequent words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "NUM_WORDS = 20000\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=NUM_WORDS)\n",
    "\n",
    "print(\"Loading data...\")\n",
    "print(len(x_train), \"train sequences\")\n",
    "print(len(x_test), \"test sequences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_train` and `x_test` are `numpy.ndarray`'s containing list of sequences.\n",
    "\n",
    "A few examples are show below: the samples don't have the same length and are encoded by integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be a valid way to represent text for machines, but it's not really usefull for humans.\n",
    "Let's try to get the original text back.\n",
    "\n",
    "The `imdb` module ships with a function `get_word_index()` to decode the integers to words, but we'll have to do some extra work: there are some special words for that are not the word index.\n",
    "See the arguments `start_char`, `oov_char` and `index_from` of the function `imdb.load_data()` for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_FROM = 3  # First actual word.\n",
    "\n",
    "word_index = imdb.get_word_index()\n",
    "word_to_index = {k: (v + INDEX_FROM) for k, v in word_index.items()}\n",
    "# Add special words.\n",
    "word_to_index[\"<PAD>\"] = 0  # Padding\n",
    "word_to_index[\"<START>\"] = 1  # Starting of sequence\n",
    "word_to_index[\"<UNK>\"] = 2  # Unknown word\n",
    "\n",
    "index_to_word = {v: k for k, v in word_to_index.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dictionary `index_to_word` we can display the original reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I_SHOW = 4\n",
    "\n",
    "\" \".join([index_to_word[w] for w in x_train[I_SHOW]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise \n",
    "> \n",
    "> Play around with `I_SHOW` and read some other reviews!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section showed that the text was encoded by integers, but we need to do some more processing: `keras` needs all sequences (/reviews) to be of equal length.\n",
    "\n",
    "We can choose to pad all sequences to the longest length, or we can choose a maximum review length and cut longer reviews.\n",
    "We'll cut reviews after `MAXLEN=80` words and pad them if needed: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "MAXLEN = 80\n",
    "\n",
    "X_train = sequence.pad_sequences(x_train, maxlen=MAXLEN)\n",
    "X_test = sequence.pad_sequences(x_test, maxlen=MAXLEN)\n",
    "\n",
    "print(\"Size of X_train\", X_train.shape)\n",
    "print(\"Size of X_test\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a valid threshold?\n",
    "\n",
    "The figure below shows that we'll be cutting most texts and padding only some.\n",
    "However, this can be fine: most of the sentiment could be in the first 80 words.\n",
    "If we find out that it's not enough, we'll come back and increase the text length!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(s) for s in x_train]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "sns.kdeplot(lengths, cumulative=True, label=\"x_train\", ax=ax)\n",
    "ax.plot((80, 80), ax.get_ylim(), \"--k\")\n",
    "ax.set_xlabel(\"Sequence lengths [# words]\")\n",
    "ax.set_ylabel(\"Cumulative fraction\")\n",
    "ax.set_title(\"Occurence of sequence lengths in reviews\")\n",
    "ax.legend([\"x_train\", \"Max length\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now done with the preprocessing!\n",
    "In our training set we have 25000 reviews of 80 words (some of the padded).\n",
    "All words are encoded by integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Size of X_train:\", X_train.shape)\n",
    "\n",
    "X_train[3, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done with all the data manipulation, how is our neural network going to look like?\n",
    "Our initial model will consist of three layers: an embedding layer, a recurrent layer and a dense layer.\n",
    "The embedding layer learns the relations between words, the recurrent layer learns what the document is about and the dense layer translates that to sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Embedding layer\n",
    "\n",
    "The embedding layer will embed our original word vectors in a dense, lower-dimensional space.\n",
    "This embedding can capture complicated relationships between words and make it easier to learn.\n",
    "\n",
    "We'll see in a minute what we mean with that, let's first start with the traditional approach of __one-hot encoding__.\n",
    "One-hot encoding words indexes words and represents them as a big vectors with zeros and ones.\n",
    "\n",
    "\n",
    "With one-hot encoding, the vocabulary \"$\\textsf{code - console - cry - cat - dog}$\" would be represented like this:\n",
    "\n",
    "![](../images/one_hot_encoding.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three text snippets \"$\\textsf{code console}$\", \"$\\textsf{cry cat}$\" and \"$\\textsf{dog}$\" are represented by combining these word vectors:\n",
    "\n",
    "![](../images/one_hot_document.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This representation has some problems.\n",
    "\n",
    "This matrix will be very large for large vocabulary and also very empty.\n",
    "Many statistical models have problems learning from such big and sparse data. \n",
    "There are __too many features__ to learn from and __not enough samples__ to understand every feature.\n",
    "Combining words in an intelligent way could solve this.\n",
    "\n",
    "Treating words as __atomic units__ throws away a lot of information.\n",
    "\"$\\textsf{cat}$\" is more similar to \"$\\textsf{dog}$\" than to \"$\\textsf{code}$\", and \"$\\textsf{console}$\" has a different meaning when occuring next to \"$\\textsf{code}$\" than when it's next to \"$\\textsf{cry}$\".\n",
    "These complex relationships cannot be represented by our simple one-hot encoding.\n",
    "\n",
    "Instead of learning from one-hot encoding, we first let the neural network __embed__ words in a smaller, continuous vector space where similar words are close to each other.\n",
    "The smaller space makes it easier to learn from and a continuous representation allows to learn complex relationships.\n",
    "\n",
    "Such an embedding for our vocabulary could look like this: \n",
    "\n",
    "![](../images/embedding_encoding.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only need two dimensions for our words instead of five, \"$\\mathsf{cat}$\" is close to \"$\\mathsf{dog}$\", and \"$\\mathsf{console}$\" is somewhere between \"$\\mathsf{code}$\" and \"$\\mathsf{cry}$\".\n",
    "Closeness in this space indicates similarity.\n",
    "Encoding our documents with the average of their word vectors also makes a lot of sense:\n",
    "![](../images/embedding_document.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet \"$\\textsf{dog}$\" is now closer to \"$\\textsf{cry cat}$\" than to \"$\\textsf{code console}$\".\n",
    "How is this different than the one-hot encoding?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These vectors are a thus __lower dimensional__, __denser representation__ of our words and they also capture __semantic information__ about words and their relationships to another. \n",
    "Certain directions in the vector space embed certain semantic relationships such as male-femal, verb-tense and country-capital relationships between words.\n",
    "\n",
    "<img src=\"../images/linear-relationships.png\" alt=\"Drawing\" style=\"width: 90%;\"/>\n",
    "\n",
    "> Source: https://www.tensorflow.org/tutorials/word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build an embedding layer in `keras` using `keras.layers.Embedding`.\n",
    "`keras` can learn this layer for you, but you can also pretrained embeddings generated by others.\n",
    "More on this later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Recurrent layer\n",
    "\n",
    "__Recurrent Neural Nets__ naturally deal with word order because they can go over a __sequence__ of words and keep a __memory__ of the information that has been calculated so far.\n",
    "\n",
    "This could help when trying to assign sentiment to sentences, as shown in the figure below.\n",
    "A word can trigger a sentiment that carries on for one or multiple sentences.\n",
    "\n",
    "<img src=\"../images/sentiment-neuron.gif\" style=\"width: 75%;\"/>\n",
    "\n",
    "\n",
    "> Source: [Unsupervised Sentiment Neuron](https://blog.openai.com/unsupervised-sentiment-neuron/)\n",
    "\n",
    "If we'd be interested in __understanding a document__ like in the previous example, we could use the following architecture:\n",
    "\n",
    "<img src=\"../images/rnn-architecture.png\" style=\"width: 75%;\"/>\n",
    "\n",
    "> Source: [Goodfellow, 2016]\n",
    "\n",
    "The left side of the figure shows a short-hand of the neural network, the right side shows the unrolled version.\n",
    "In the figure we have:\n",
    "\n",
    "* $\\mathbf{x}^{(t-1)}$, $\\mathbf{x}^{(t)}$, $\\mathbf{x}^{(t+1)}$: input word vector at time $t$.\n",
    "* $\\mathbf{h}^{(t-1)}$, $\\mathbf{h}^{(t)}$, $\\mathbf{h}^{(t+1)}$: output of the previous time-step $t-1$.\n",
    "\n",
    "At each time-step, the input is the output of the previous time-step $\\mathbf{h}^{(t-1)}$ and a new input word vector $\\mathbf{x}^{(t)}$.\n",
    "Over time we adjust our idea of the document $\\mathbf{h}^{(t)}$ until we've seen all words in the document.\n",
    "This is illustrated in the figure below: we get a new word vector at each time-step and __carry over a score__.\n",
    "\n",
    "The final score $\\mathbf{h}^{(T)}$ represents what the neural network has learned about the document after having seen every word.\n",
    "We could, for instance, use the final scores to detect sentiments - and that's exactly what we'll be doing!\n",
    "\n",
    "We'll use a specific kind of recurrent layer: a LSTM.\n",
    "The Long Short Term Memory neuron are able to learn long-term dependencies and often perform better than standard RNNs.\n",
    "Read [this blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) if you'd like more info.\n",
    "\n",
    "The LSTM layer can be found in `keras.layers.LSTM`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Dense layer\n",
    "\n",
    "The first layer learns a good representation of words, the second learns to combine words in a single idea, and the final layer turns this idea into a classification.\n",
    "We will use a simple dense layer from `keras.layers.Dense` that transforms the idea vectors into a 0 or 1.\n",
    "The layer will consist of a single neuron that takes all connections and outputs 0 or 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we now how the architecture looks like and we have our data in `X_train` and `X_test`, it's time to build a model.\n",
    "\n",
    "> #### Exercise: LSTM for sentiment classification\n",
    ">\n",
    "> * Build a sequential model with three layers: embedding, LSTM and dense layers. \n",
    ">     * Don't make the embedding layer *larger than* 200 units. Use `NUM_WORDS` as the vocabulary size.\n",
    ">     * Use *at most* 256 LSTM units. Play around with parameters `dropout` and `recurrent_dropout`.\n",
    "> * Compile the model with `'binary_crossentropy'` as the loss and use `'accuracy'` as validation metric.\n",
    "> * Add callbacks to the fitting: use `keras.callbacks.ModelCheckpoint()` and `keras.callbacks.EarlyStopping()`.\n",
    "> * Reasonable test scores are 0.42 for the binary cross-entropy and and 0.81 for the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load ../answers/sentiment_lstm.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you reached the benchmarks, you've succesfully trained a recurrent neural network for text classification!\n",
    "\n",
    "The next section gives some pointers what to do next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Going deeper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've only touched the surface on applying RNNs to text.\n",
    "This section contains some more exercises to deepen your understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Baseline\n",
    ">\n",
    "> This dataset is small, so might not really benefit from the complexity from deep learning.\n",
    "> Use [`sklearn`](http://scikit-learn.org/stable/) to create a baseline: create a `Pipeline` using the `TfidfVectorizer()` and `BernoulliNB()` \n",
    "> What's your best score?\n",
    ">\n",
    "> Hint:\n",
    "> - Use `X_train_translated` and `X_test_translated` from below; `TfidfVectorizer()` works better with real strings than integers.\n",
    "> You can still use `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indices to text.\n",
    "X_train_translated = [\" \".join(index_to_word[w] for w in s) for s in x_train]\n",
    "X_test_translated = [\" \".join(index_to_word[w] for w in s) for s in x_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/sentiment_sklearn.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Minimum Viable Network\n",
    "> \n",
    "> You probably don't need a big network for this small dataset: there's not enough data to learn really complex relations.\n",
    "What's the smallest network you still get good results with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/sentiment_mvn.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Visualizing Embeddings\n",
    ">\n",
    "> Section 3 argued that the embeddings would learn relations between words.\n",
    "However, we didn't prove this for this solution: we just provided an architecture for the network and told the network to learn sentiment.\n",
    "This means that it didn't necessarily use the embedding to learn a representation that makes sense to us: everything was conditioned on sentiment classfication.\n",
    ">\n",
    "> Visualize the embeddings: are they any good?\n",
    "> * Get the weights from the right layer:\n",
    "    * See the attribute `.layers` of the network.\n",
    "    * Use the method `.get_weights()` of the layer.\n",
    "> * Use [TSNE](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) to map the weights into a 2D representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/sentiment_tsne.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### Exercise: Transfer Learning\n",
    ">\n",
    "> Instead of training the embeddings, you can use word embeddings pretrained on a large corpus.\n",
    "This allows you to leverage complex relations learned from large corpora on your smaller datasets.\n",
    ">\n",
    "> Read [this Keras blog](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) and use word embeddings from GloVe.\n",
    "Note that not all words may be present, so you'll have to do some preprocessing.\n",
    "Does this improve your model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
